{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization\n",
    "\n",
    "## LLMs as distributions\n",
    "\n",
    "* LLM can be viewed as probability distribution $\\pi(y|x)$ over possible responses $y$ for a given query $x$.\n",
    "* Sampling responses -> rather than a single deterministic output, LLMs sample tokens from a probability distribution at each step.\n",
    "* Dataset approximation -> $X \\sim D$ indicates that the training data distribution $D$ underpins how the model assigns probabilities to queries $X$.\n",
    "\n",
    "### Transformers & softmax probabilities\n",
    "\n",
    "* Token by token generation -> For a query, the transformer predicts the next token using a distribution derived from the softmax function.\n",
    "* Softmax function -> Transforms logit into probabilities for each token in the vocab.\n",
    "* Sequential dependence -> The probability distribution at time $t+1$ depends on the generated token at time $t$. Past tokens influence future token probabilities.\n",
    "\n",
    "### Sampling & generation process\n",
    "\n",
    "* Initial Query - > convert words into tokenized embeddings\n",
    "* Transformer Pass -> model outputs a probability distribution over the next token\n",
    "* Random Selection -> pick a token based on the distribution (not necessarily the highest probability)\n",
    "* Iterate -> Feed the chosen token back into the model and repeat until the sequence is complete or \\<eos\\> is generated.\n",
    "\n",
    "### Generation parameters\n",
    "\n",
    "* temperature\n",
    "    * scales the logits before applying softmax\n",
    "    * effect\n",
    "        * hight temp -> more uniform distribution -> more randomness\n",
    "        * low temp -> sharper distribution -> less randomness\n",
    "* top-k sampling\n",
    "    * restricts next-token choices to the top k highest prob tokens\n",
    "    * process\n",
    "        * compute probs\n",
    "        * keep only top K tokens\n",
    "        * re-normalize to ensure probabilities sum to 1\n",
    "    * effect\n",
    "        * limits randomness to the most likely tokens\n",
    "* top-p sampling\n",
    "    * chooses the smallest set of tokens whose cumulative prob exceeds p\n",
    "    * effect\n",
    "        * allows dynamic selection of the token set, ensuring coverage of high-prob mass\n",
    "* beam search\n",
    "    * keeps track of multiple top candidate sequences and expands them step by step\n",
    "    * effect\n",
    "        * systematically searches for high prob sequences instead of just sampling\n",
    "* repetition penalty\n",
    "    * penalizes repeated tokens or phrases to encourage diversity in the output\n",
    "    * effect\n",
    "        * reduces repetitive loops (\"the the the...\")\n",
    "* max & min tokens\n",
    "    * sets upper and lower limits for the number of generated tokens\n",
    "    * effect\n",
    "        * controls how long the generated text can be\n",
    "\n",
    "### Summary\n",
    "\n",
    "* LLMs can be perceived as stochastic generators, guided by the learned distribution\n",
    "* Softmax & transformers are the core mechanisms for assigning probs at the generation step\n",
    "* Parameters\n",
    "    * Temperature -> alter randomness\n",
    "    * Top K & Top P -> control token selection strategies\n",
    "    * Beam search -> systematically explores multiple high-prob sequences\n",
    "    * Repetition penalty & response length -> refine structure and lenght of the outputs\n",
    "* Adjusting the parameters helps to tailor the outputs\n",
    "\n",
    "\n",
    "## From distributions to policies\n",
    "\n",
    "### Policy in reinforcement learning\n",
    "\n",
    "* a policy maps the agent's current state to the action it should take\n",
    "* a role of randomness -> policy often include stochasticity to explore unseen possibilities, leading to more robust and adaptable results\n",
    "* application -> in RL tasks, policies guide action sequences to achieve goals or maximize rewards\n",
    "\n",
    "### LLMs as policies/distributions\n",
    "\n",
    "* language model can be viewed as a policy $\\pi(y|X)$ where given an input $x$, the model produces a distribution over possible outputs $y$\n",
    "* text generation -> the policy guides the model in exploring diverse text output paths, enhancing creativity and context relevance\n",
    "\n",
    "### Roll-outs & policy distribution\n",
    "\n",
    "* roll-outs in LLM\n",
    "    * when a query is provided, the model generates multiple possible responses (roll-outs)\n",
    "    * each rollout is a distinct sequence of tokens sampled from the policy distribution\n",
    "* roll-outs in RL\n",
    "    * roll-outs also involve reward signals that guide future policy updates\n",
    "    * note -> LLM frameworks like HG may refer to rollout differently than RL, typically without explicit rewards\n",
    "\n",
    "### Mapping $y$ from $x$ via a policy $pi$\n",
    "\n",
    "* $y \\sim \\pi(y|X)$ indicates the output $y$ is sampled from a distribution conditioned on $x$\n",
    "* sequential probs -> the model breaks down the probability of a sentence into a product of conditional probs over tokens (transformer + softmax)\n",
    "\n",
    "### Summary\n",
    "\n",
    "* policy as distribution -> in both RL and LLM, a policy/distribution guides possible actions (tokens)\n",
    "* viewing LLMs as policies highlights their capacity for exploration and action selection (text-generation)\n",
    "* multiple possible outputs for a given input, in RL this is reward-based, while in LLM context, just different responses\n",
    "\n",
    "## Reinforcement learning with human feedback\n",
    "\n",
    "* the concept uses human feedback (rewards) to guide and fine-tune a pre-trained language model\n",
    "* monkeys typing at random eventually produce coherent text, but banana can accelerate the creation of desired output\n",
    "\n",
    "### Reward function $r(X,Y)$\n",
    "\n",
    "* a function assigning a score to the response $Y$ given a query $X$\n",
    "* indicates quality or relevance of the model's output and provides a basis for learning\n",
    "* example\n",
    "    * query -> Which country owns Antarctica?\n",
    "    * response 1 ->  ?9dfsa -> reward: 0 (irrelevant)\n",
    "    * response 2 -> No country owns Antarctica -> reward .9 (mostly correct)\n",
    "    * response 3 -> Antarctica is governed by an international treaty -> reward 1 (ideal)\n",
    "\n",
    "### Rollouts in RLHF\n",
    "\n",
    "* a rollout is query - response pair, multiple roll-outs help the model explore diverse responses\n",
    "* for each query, the model can produce several distinct responses, each receiving a reward, which are then used to adjust model parameters\n",
    "\n",
    "### Expected reward $E(r(X,Y))$\n",
    "\n",
    "* empirical estimate -> sum or average rewards across multiple queries and responses to approximate how well the model is performing\n",
    "* $\\hat R \\sim \\frac{1}{N \\times K} \\sum_{n=1}^N \\sum_{k=1}^N r(X_n, Y_{n,k})$\n",
    "    * $N$ stands for number of queries, $K$ for number of responses per query (roll-outs)\n",
    "    * $r(X_n, Y_{n,k})$ reward for the $k$-th response to the $n$-th query\n",
    "* policy perspective -> the expected reward is an expectation over the data (queries) and the model's distribution\n",
    "\n",
    "### Fine-tuning with a reward model\n",
    "\n",
    "* setup\n",
    "    * agent/policy -> the LLM with parameters $\\theta$\n",
    "    * reward model -> evaluates query-response pairs and returns a reward\n",
    "\n",
    "* process\n",
    "    * step 1 -> input query $X$ to the LLM -> generate response $Y$\n",
    "    * step 2 -> reward model takes $(X,Y)$ -> outputs reward $r(X,Y)$\n",
    "    * step 3 -> LLM updates parameters $\\theta$ to maximize reward signal\n",
    "\n",
    "* example\n",
    "    * query -> Which country owns Antarctica?\n",
    "    * responses -> ?9dfsa, No country...\n",
    "    * highest reward -> Antarctica is governed by a international treaty ->1\n",
    "    * outcome -> the model learns to favor this type of correct response\n",
    "\n",
    "### Summary\n",
    "\n",
    "* human feedback drives learning, assigning rewards for correct responses focuses the model on producing better results\n",
    "* roll-outs -> each q-r pair is evaluated, guiding how params are updated\n",
    "* expected reward -> gives an overall measure of the model's performance\n",
    "* RLHF -> balances exploration (response sampling) with exploitation (updating the model's params)\n",
    "\n",
    "\n",
    "## Policy gradient foundations\n",
    "\n",
    "* a policy $\\pi_0$ assigns probabilities to potential actions or responses, given an input query $X$\n",
    "* objective -> maximize the expected reward $E[r(X,Y)]$ over query-response pairs $(X,Y)$\n",
    "\n",
    "### Proximal policy optimization\n",
    "\n",
    "* a method to update policy parameters $\\theta$ with stability, avoiding large destabilizing changes\n",
    "* components\n",
    "    * clipped surrogate objective -> prevents the new policy from diverging excessively from the old one\n",
    "    * KL penalty coefficient ($\\Beta$) -> regularizes the policy update by penalizing high divergence between the old and the new policies\n",
    "\n",
    "### Training process\n",
    "\n",
    "* agent & reward model\n",
    "    * and agent (LLM) with learnable parameters $\\theta$ generates a response $Y$ to a query $X$\n",
    "    * reward function/model evaluates $(X,Y)$ and returns a scalar reward $r(X,Y)$\n",
    "\n",
    "* roll-outs\n",
    "    * the combination $(X,Y)$ is often referred to as a rollout\n",
    "    * multiple roll-outs across queries help in estimating the expected reward\n",
    "\n",
    "* objective\n",
    "    * to find parameter set $\\theta$ that maximizes the expected reward\n",
    "    * a reference model can be included as a regularization term, ensuring we dont deviate to much from the original model\n",
    "\n",
    "### Log-derivative trick\n",
    "\n",
    "* directly computing $\\nabla_0 \\ E[r(X,Y)]$ can be intractable, the log-derivative trick reformulates the expression for easier gradient estimation ->\n",
    "    * express the objective as an expectation of rewards under the policy distribution\n",
    "        * $E[r_Y|\\theta] = \\sum_Y \\ r(X,Y)\\ \\pi_0(Y|X)$\n",
    "        * $\\hat \\theta = arg \\ max_\\theta [\\sum_Y \\ r(X,Y)\\ \\pi_0(Y|X)]$\n",
    "        * $\\nabla_\\theta E[r_Y|\\theta] = \\sum_Y r(X,Y)\\nabla_\\theta \\pi_0(Y|X)$\n",
    "    * introduce $log \\pi_0(Y|X)$\n",
    "        * $\\nabla_\\theta log(\\pi_0(Y|X)) = \\frac{\\nabla_\\theta \\pi_0(Y|X)}{\\pi_0(Y|X)}$\n",
    "    * rearrange and factor out the gradient, enabling Monte Carlo sampling to compute updates\n",
    "        * $\\nabla_\\theta \\pi_0(Y|X) = \\nabla_\\theta log(\\pi_0(Y|X)) \\pi_0(Y|X)$\n",
    "        * $ \\nabla_\\theta E[r_Y|\\theta]  = E_{Y\\sim \\pi_0(Y|X)} [r(X,Y) \\nabla_\\theta \\pi_0(Y|X)]$\n",
    "        * $ E_{X \\sim D}[\\nabla_\\theta E[r_Y|\\theta]] = \\nabla_\\theta E_{X \\sim D}[E[r_Y|\\theta]]$\n",
    "\n",
    "### Practical tips\n",
    "\n",
    "* regular evaluation with human feedback\n",
    "* moderate KL penalty to avoid overly large or small regularization (instability vs no-updates)\n",
    "* temperature tuning -> go from lower (more exploitation) to higher (more exploration)\n",
    "\n",
    "### Summary\n",
    "\n",
    "* policy gradient framework to directly optimize policy $\\pi_0$ by maximizing expected reward\n",
    "* PPO introduces tools to keep policy updates proximal and stable\n",
    "* log-derivative trick is a RL foundational trick for computing gradient updates\n",
    "* model fine-tuning through generating roll-outs, computing rewards, updating $\\theta$, always balancing performance and stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Direct Preference Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition function\n",
    "\n",
    "* direct preference optimization is a reinforcement learning technique that fine-tunes models based on human preferences more directly than traditional methods\n",
    "* core ideas\n",
    "    * collect data on human preferences by comparing different model outputs\n",
    "    * directly optimize the model's parameters so outputs are better aligned with human preference\n",
    "\n",
    "### DPO vs traditional RL\n",
    "\n",
    "* traditional\n",
    "    * often uses a reward function indirectly related to human feedback\n",
    "    * requires an actor (policy) and critic (value/reward estimator) eg. PPO\n",
    "* DPO\n",
    "    * directly incorporates preference data into optimization\n",
    "    * aims to avoid the complexity of training reward models by leveraging direct comparisons between responses\n",
    "\n",
    "### Three components\n",
    "\n",
    "* reward function (encoder)\n",
    "    * evaluates the relevance or quality of a response\n",
    "    * example \"this is a cat\" -> low score if discussing LLMs, high score if relevant\n",
    "* target decoder (parameters $\\theta$)\n",
    "    * the model to be fine-tuned (policy $\\pi$)\n",
    "* reference model\n",
    "    * acts as a baseline or \"initial\" model to regularize ho far the new policy can deviate\n",
    "\n",
    "### Partition function & normalization\n",
    "\n",
    "* partition function ($Z$)\n",
    "    * ensures that the sum of probabilities over all possible outcomes is 1\n",
    "    * converts unnormalized positive functions into valid probability distributions through scaling\n",
    "\n",
    "* logistic example\n",
    "    * $\\sigma(x)$ -> sigmoid mapping $x$ to $(0,1)$\n",
    "    * $P(y=1|x) = 1- \\sigma(x)$\n",
    "    * partition function is implicitly accounted for through the sigmoid shape\n",
    "\n",
    "### Key DPO steps\n",
    "\n",
    "* collect preference data -> show model outputs to humans, gather \"Which is better?\" judgments\n",
    "* construct objective -> use these preferences (and the ref model) to form an objective function that pushes the policy to align with top choices\n",
    "* normalize & fine-tune -> employ the partition function to ensure outputs form a valid distribution, optimize with gradient-based methods\n",
    "\n",
    "### Summary\n",
    "\n",
    "* DPO is more directly aligned with human choices, less complex than multi-stage RL setups\n",
    "* partition function is critical to transform raw scores into proper probability distributions, appears in both logreg and more general RL-based preference learning contexts\n",
    "* DPO is used for fine-tuning, preference based training in language tasks etc\n",
    "\n",
    "## Optimal solution\n",
    "\n",
    "### Objective functions\n",
    "\n",
    "* an objective function measures how far a model's predictions deviate from targets, it guides model training, optimization of the objective improves performance\n",
    "\n",
    "### KL divergence\n",
    "\n",
    "* measures dissimilarity between two probability distributions $\\pi * (y|x)$ and $\\pi_{ref} (y|x)$\n",
    "* zero divergence if the distributions are identical\n",
    "* asymmetrical\n",
    "* usage\n",
    "    * minimizing KL divergence aligns a new policy $\\pi *$ with a reference policy $\\pi_{ref}$\n",
    "\n",
    "### Converting max problem to min problem\n",
    "\n",
    "* negation trick -> turning $arg \\ max f(w)$ into $arg \\ min [-f(x)]$\n",
    "* scalar multiplication does not affect the location of the optimum, only rescales the func\n",
    "\n",
    "### From RL objective to DPO\n",
    "\n",
    "* initial RL setup\n",
    "    *   $max_{\\pi *} E[r(x,y)] - \\beta \\ D_{KL} ( \\pi_* (y|x)|| \\pi_{ref} (y|x))$\n",
    "\n",
    "* reformulation\n",
    "    * multiply by -1, rearrange terms and convert into an expectation\n",
    "    * express the reward term as a log exponential to combine with $log \\pi_{ref}$\n",
    "\n",
    "* partition func & normalization\n",
    "    * introduce partition function $Z(x)$ to ensure probs sum to 1, ie\n",
    "        * $\\pi * (y|X) = \\frac{\\pi_{ref} (y|x) \\ exp(\\frac{1}{\\beta} r(x,y))}{Z{x}}$\n",
    "        * where $Z(x) = \\sum_y \\pi_{ref}(y|x) \\ exp(\\frac{1}{\\beta} r(x,y))$\n",
    "\n",
    "\n",
    "### Optimal DPO policy\n",
    "\n",
    "* closed form $\\pi * (y|X) = \\frac{\\pi_{ref} (y|x) \\ exp(\\frac{1}{\\beta} r(x,y))}{Z{x}}$\n",
    "* interpretation -> the new policy re-weights the reference policy by $exp(\\frac{1}{\\beta} r)$\n",
    "* $\\beta param$\n",
    "    * controls how much we amplify the reward term relative to $\\pi_{ref}$\n",
    "    * larger $\\beta$ -> stronger emphasis on reward, smaller -> closer to the reference policy\n",
    "\n",
    "### Partition function complexity\n",
    "\n",
    "* $Z(x) = \\sum_{Y \\in V^T} \\pi_{ref}(y|x) \\ exp(\\frac{1}{\\beta} r(x,y))$, where $V^T$ is the set of all possible sequences of length $T$\n",
    "* exponential growth\n",
    "    * for large vocabularies and longer sequences, the partition func sum becomes huge\n",
    "    * direct computation is typically impractical, motivating approximate or sampling-based methods\n",
    "\n",
    "### Summary\n",
    "\n",
    "* KL divergence & reward balances alignment with a reference model ($\\pi_{ref}$) and maximizing a reward function $r$\n",
    "* closed form solution $\\pi * (y|X) = \\frac{\\pi_{ref} (y|x) \\ exp(\\frac{1}{\\beta} r(x,y))}{Z{x}}$\n",
    "* using $\\beta$ as tuning param to manage policy stability and reward func\n",
    "* estimating complex partition function might not be feasible\n",
    "\n",
    "\n",
    "## From PPO to DPO\n",
    "\n",
    "* DPO goal is to fine-tune a causal language model on pairwise preference data, bypassing the complexity of traditional RL-based methods like PPO\n",
    "\n",
    "### Ranking vs scoring\n",
    "\n",
    "* it is hard for humans to numerically score responses, this challenge is addressed with pairwise comparison which are simple and still informative\n",
    "* Bradley-Terry model is a classical statistical model for pairwise comparisons used here to to guide the preference-based fine tuning\n",
    "\n",
    "### Bradley-Terry loss\n",
    "\n",
    "* $l(\\theta) = -ln \\ \\sigma(s(A)-s(B))$, where $\\sigma$ is the logistic function and $s(.)$ is the score\n",
    "* dataset notation\n",
    "    * $X$ -> query, $Y_w$ -> winning response, $Y_l$ -> loosing response\n",
    "    * summation or expectation over $(X,Y_w,Y_l)$ samples in the dataset $D$\n",
    "\n",
    "### DPO optimal policy & reference model\n",
    "\n",
    "* $\\pi * (y|X) = \\frac{\\pi_{ref} (y|x) \\ exp(\\frac{1}{\\beta} r(x,y))}{Z{x}}$, where $r$ is the reward function, $\\pi_{ref}$ is a reference policy, and $Z(x)$ is the partition function\n",
    "* computing $Z(x)$ is not practical, taking log ratios of winning vs losing responses, the partition func cancels out\n",
    "\n",
    "### From policy ratio to loss\n",
    "\n",
    "* log ratio trick\n",
    "    * take the difference between the log probs of winnings vs losing responses\n",
    "    * combine it with a form of the reward (or equivalently the LLM outputs + reference model weights)\n",
    "\n",
    "* Bradley-Terry-like loss\n",
    "    * the new loss depends only on the ratio $\\frac{\\pi_\\theta(Y_w|X)}{\\pi_\\theta(Y_l|X)}$ and the reference model, effectively removing need for an explicit reward function or partition function\n",
    "\n",
    "### Simplified expression & plot\n",
    "\n",
    "* set $\\beta =1$ and reference policy to a constant $C$\n",
    "* define $u = \\frac{\\pi_\\theta(Y_w|X)}{\\pi_\\theta(Y_l|X)}$\n",
    "* as $u$ increases the model is more likely to favor the winning response, loss decreases as $u$ grows above 1, encouraging correct ranking\n",
    "\n",
    "### Converting loss to cost\n",
    "\n",
    "* BT model -> $l = -ln \\ \\sigma (log(\\pi_\\theta (Y_w|X))-log(\\pi_\\theta (Y_l|X)))$\n",
    "* transformation\n",
    "    * by re-expressing the above as a negative lo-likelihood, we get a cost func that is differentiable wrt $\\theta$\n",
    "    * can be implemented through pytorch or HF DPO trainer\n",
    "\n",
    "### Summary\n",
    "\n",
    "* DPO replaces complex PPO with a direct optimization objective derived from pairwise preference\n",
    "* partition function can be eliminated through log ratios of winning and loosing responses, that cancel out normalization term\n",
    "* BT insights shows that pairwise comparison is simpler for humans and yields a straightforward log-loss for optimization\n",
    "* using the log ratios reduces computational resources required\n",
    "* resulting diff func -> $r(X,Y_w)-r(X,Y_l) = \\beta ln (\\frac{pi_{r}(Y_w|X)}{\\pi_{ref}(Y_w|X)}) - \\beta ln (\\frac{pi_{r}(Y_l|X)}{\\pi_{ref}(Y_l|X)})$\n",
    "* resulting loss func -> $-\\sigma (\\beta ln (\\frac{pi_{r}(Y_w|X)}{\\pi_{ref}(Y_w|X)}) - \\beta ln (\\frac{pi_{r}(Y_l|X)}{\\pi_{ref}(Y_l|X)}))$\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
