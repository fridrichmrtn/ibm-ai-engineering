{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "## Introduction\n",
    "* natural language processing, computer vision, time series\n",
    "* self-attention to process data in parallel\n",
    "* self-attention\n",
    "    * enables each input token to attend to every other token (incl. self)\n",
    "    * caputers context and relationships\n",
    "    * represented by query, key, value\n",
    "    * attention score is matmul of query and key, which is scaled\n",
    "    * the score is used to re-weight value vectors\n",
    "* architecture\n",
    "    * encoder\n",
    "        * multiple layers\n",
    "            * self-attention\n",
    "            * feed-forward net\n",
    "            * residual conections\n",
    "            * layer normalization\n",
    "        * input embedded and passed through positional encoding    \n",
    "    * decoder\n",
    "        * cross atetntion mechanism to the enc output\n",
    "        * generaters sequences based on the context provided by enc\n",
    "        * target sequence as input\n",
    "        * applies self-attention and cross attention with the enc output\n",
    "        * passes through the feed-forward neural-net\n",
    "    * see code example in the jupyter notebook\n",
    "    \n",
    "## Sequential data\n",
    "\n",
    "\n",
    "## Advanced applications\n",
    "\n",
    "## Time-series\n",
    "\n",
    "## Tensorflow for sequential data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
