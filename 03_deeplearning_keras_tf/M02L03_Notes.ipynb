{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "## Introduction\n",
    "* natural language processing, computer vision, time series\n",
    "* self-attention to process data in parallel\n",
    "* self-attention\n",
    "    * enables each input token to attend to every other token (incl. self)\n",
    "    * caputers context and relationships\n",
    "    * represented by query, key, value\n",
    "    * attention score is matmul of query and key, which is scaled\n",
    "    * the score is used to re-weight value vectors\n",
    "* architecture\n",
    "    * encoder\n",
    "        * multiple layers\n",
    "            * self-attention\n",
    "            * feed-forward net\n",
    "            * residual conections\n",
    "            * layer normalization\n",
    "        * input embedded and passed through positional encoding    \n",
    "    * decoder\n",
    "        * cross atetntion mechanism to the enc output\n",
    "        * generaters sequences based on the context provided by enc\n",
    "        * target sequence as input\n",
    "        * applies self-attention and cross attention with the enc output\n",
    "        * passes through the feed-forward neural-net\n",
    "    * see code example in the jupyter notebook\n",
    "    \n",
    "## Sequential data\n",
    "* transformers, can capture long-range dependencies and can run in parallel, which is large improvement over RNNs\n",
    "* applications: NLP, time-series\n",
    "* self-attention mechanism is at core of transformers architecture\n",
    "* architecture\n",
    "    * encoder\n",
    "        * processes the input sequence\n",
    "        * multi-head self attention\n",
    "        * feed-forward network\n",
    "        * residual connection\n",
    "        * layer normalization\n",
    "    * decoder\n",
    "        * generates the output sequence\n",
    "\n",
    "## Advanced applications\n",
    "* transformers are used in other applications outside of NLP or time-series\n",
    "    * vision transformers\n",
    "        * break-down images into patches and learn sequence of patches\n",
    "        * sequence of patches into transformer\n",
    "    * speech recognition\n",
    "        * audio-signals into spectrograms, spectrograms into patches\n",
    "        * sequence of patches into the transformer\n",
    "    * reinforcement learning\n",
    "        * captures complex dependencies in sequence of states and actions\n",
    "        * more efficient learning\n",
    "\n",
    "## Time-series\n",
    "* long-range dependencies captured more efficiently\n",
    "* parallel processing\n",
    "* variable-length sequences and missing data are not a problem\n",
    "* architecture\n",
    "    * embedding layer\n",
    "    * transformer blocks\n",
    "        * multi-head attention\n",
    "        * feed-forward neural net\n",
    "    * dense layer for predictions\n",
    "\n",
    "## Tensorflow for sequential data\n",
    "* layers for capturing temporal dependencies > RNNs, LSTMs, GRU, Conv1D\n",
    "* text data > tokenization & padding through tf"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
