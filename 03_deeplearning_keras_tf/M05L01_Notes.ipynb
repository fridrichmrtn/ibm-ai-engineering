{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Keras techniques\n",
    "* custom training loops\n",
    "    * customize training process,\n",
    "    * complex training strategies,\n",
    "    * custom loss functions.\n",
    "* custom layers\n",
    "    * dense and lambda layers\n",
    "    * custom activation functions\n",
    "* custom callbacks\n",
    "    * monitor training\n",
    "    * early stopping\n",
    "    * model checkpoints\n",
    "    * custom callbacks\n",
    "* model optimization\n",
    "    * mixed-precision training (reduce memory footprint)\n",
    "    * optimization toolkit\n",
    "\n",
    "## Custom training loops\n",
    "* components > dataset, model, optimizer, loss\n",
    "* tape records the operations in the forward pass for autodiff for the back pass\n",
    "* more control over the process, logging and monitoring, flexibility, other custom artifacts (loss & metrics)\n",
    "\n",
    "## Hyperparameter tuning\n",
    "* optimize model performance through finding best hyperparams\n",
    "* search algorithms -> random, hyperband, Bayes\n",
    "\n",
    "## Model optimization\n",
    "* weight initis > helps stable gradient flow during training\n",
    "* learning rate scheduling\n",
    "* batch normalization > stabilize the training process by scaling\n",
    "* mixed precision > mixed floats, to speedup the training and reduce memory usage\n",
    "* model pruning > removes insignificant connections and neurons\n",
    "* quantization > reduces precision of the numbers, reduces memory usage and inference time\n",
    "\n",
    "## TF optimization techniques\n",
    "* knowledge distillation -> smaller model (student) learns from larger model (teacher)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
