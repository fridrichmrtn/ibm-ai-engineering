{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning\n",
    "* **overview**\n",
    "    * agents interact with environment through available actions\n",
    "    * actions impact environment, which impacts agents through rewards\n",
    "    * rewards are unknown and must be estimated by the agent\n",
    "    * process is dynamically repeated so agents learn to estimate rewards over time\n",
    "    * agent > action > environment (state) > reward (positive actions are reinforced)\n",
    "* unlimited data and computational requirements\n",
    "* examples > games, recommendation engines, marketing, automated bidding\n",
    "* solutions represents policy, by which agents choose actions in response to the state\n",
    "* agents maximize expected rewards over time\n",
    "\n",
    "* **differs from traditional machine learning**\n",
    "    * unlike labels, rewards are not know and uncertain\n",
    "    * action impacts environment, the state changes, which changes the problem\n",
    "    * agents face a tradeoff between rewards across different periods\n",
    "\n",
    "* **approaches**\n",
    "\n",
    "## Q-learning in Keras\n",
    "* Q-learning is value-based reinforcement learning algorithm\n",
    "* trains agents to make sequences of decisions\n",
    "* learn value of specific action in a state\n",
    "* optimal action-section policy for an agent\n",
    "* Q-function\n",
    "    * provides a measure of expected utility of action `a` in state `s`\n",
    "    * follows the optimal policy\n",
    "    * updated iteratively using the Bellman equation\n",
    "        * immediate reward\n",
    "        * estimated future rewards\n",
    "        * $Q(s,a) \\leftarrow Q(s,a) + \\alpha [r+\\gamma max_{a'}\\ Q(s',a') - Q(s,a)]$\n",
    "            * $s$ - current state,\n",
    "            * $a$ - current action,\n",
    "            * $r$ - reward received after action $a$,\n",
    "            * $s'$ - state resulting from taking action $a$,\n",
    "            * $a'$ - next action,\n",
    "            * $\\alpha$ - learning rate, controlling the extent to which new information overrides the old\n",
    "            * $\\gamma$ - discount factor, modeling the importance of future rewards\n",
    "* algorithm\n",
    "    * initialize the environment and params\n",
    "        * pick a platform\n",
    "        * initialize Q-tab\n",
    "        * set hyperparameters (learning rate, discount factor, exploration rate)\n",
    "    * build Q-network\n",
    "        * construct Q-network that approximate Q-function\n",
    "    * train the Q-network\n",
    "        * implement a training loop\n",
    "    * evaluate the agent\n",
    "        * test the agent to maximize rewards\n",
    "\n",
    "* detailed algorithm\n",
    "    * initialize the environment\n",
    "        * cart pole problem from OpenAI gym (balance a pole on a cart)\n",
    "        * initialize the params (learning rate, discount factor, exploration rate)\n",
    "    * build Q-network\n",
    "        * Q-table for storing Q-values, impractical for large/continuous state-spaces\n",
    "        * Q-network approximates the table\n",
    "            * architecture\n",
    "                * 2-3 dense layers with ReLU activation\n",
    "                * input layer -> states, output layer -> actions\n",
    "    * train Q-network\n",
    "        * get initial state\n",
    "        * select action\n",
    "            * with probability of epsilon select a random action (exploration)\n",
    "            * or select the highest predicted Q-value (exploitation)\n",
    "        * take action\n",
    "            * execute the chosen action in the environment\n",
    "        * update Q-values\n",
    "            * use Bellman equation\n",
    "            * compute the target Q-value\n",
    "            * train the Q-network to minimize the difference between the predicted and target Q-value\n",
    "        * repeat\n",
    "            * reduce the exploration rate to shift from exploration to exploitation\n",
    "    * evaluate the agent\n",
    "        * interaction with the environment using the learned policy\n",
    "        * exploitation of learned Q-value patterns\n",
    "        * accumulation of total rewards over several episodes\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
