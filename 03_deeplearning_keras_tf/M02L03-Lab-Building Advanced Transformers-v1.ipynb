{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdb08ae3-6e40-4e75-900d-da6aeb43993a",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fcf9da-864f-4d9e-b0da-5f995396dd17",
   "metadata": {},
   "source": [
    "# **Lab: Building Advanced Transformers**\n",
    "\n",
    "**Estimated time needed:  30 minutes**  \n",
    "\n",
    "In this lab, you will implement and experiment with advanced Transformer models using Keras. \n",
    "\n",
    "**Learning objectives:** \n",
    "\n",
    "By the end of this lab, you will: \n",
    "\n",
    "- Implement advanced Transformer models using Keras. \n",
    "\n",
    "- Apply Transformers to real-world sequential data tasks. \n",
    "\n",
    "- Build, train, and evaluate Transformer models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f99b57-2c59-4862-8fac-9ff12df49057",
   "metadata": {},
   "source": [
    "## Step-by-Step Instructions: \n",
    "\n",
    "### Step 1: Import necessary libraries \n",
    "\n",
    "Before you start, you need to import the required libraries: TensorFlow and Keras. Keras is included within TensorFlow as `tensorflow.keras.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f75f47fd-ad84-4ee2-bc2c-6311baafb7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install tensorflow pyarrow \n",
    "#%pip install pandas  \n",
    "#%pip install scikit-learn \n",
    "#%pip install matplotlib \n",
    "#%pip install requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7b493f8-9f56-4ea0-97cf-bb6c4f3ebfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-04 11:49:48.021307: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1735987788.032832 2807423 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1735987788.036493 2807423 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-04 11:49:48.049561: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "import requests\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a6fc6c-1977-442d-a622-0794062257e1",
   "metadata": {},
   "source": [
    "####  Setup the Environment to generate synthetic stock price data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7a1644f-1983-4208-823e-1923cc2be243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a synthetic stock price dataset\n",
    "np.random.seed(42)\n",
    "data_length = 2000  # Adjust data length as needed\n",
    "trend = np.linspace(100, 200, data_length)\n",
    "noise = np.random.normal(0, 2, data_length)\n",
    "synthetic_data = trend + noise\n",
    "\n",
    "# Create a DataFrame and save as 'stock_prices.csv'\n",
    "df = pd.DataFrame(synthetic_data, columns=['Close'])\n",
    "#data.to_csv('stock_prices.csv', index=False)\n",
    "#print(\"Synthetic stock_prices.csv created and loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b9f0a73-eb0e-4c0b-adb1-2f068a5f33d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (1899, 100, 1)\n",
      "Shape of Y: (1899,)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset \n",
    "#df = pd.read_csv('stock_prices.csv') \n",
    "data = df[['Close']].values \n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "# Prepare the data for training\n",
    "def create_dataset(data, time_step=1):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(len(data)-time_step-1):\n",
    "        a = data[i:(i+time_step), 0]\n",
    "        X.append(a)\n",
    "        Y.append(data[i + time_step, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "time_step = 100\n",
    "X, Y = create_dataset(data, time_step)\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "print(\"Shape of X:\", X.shape) \n",
    "print(\"Shape of Y:\", Y.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b218abb-d401-4727-a317-2e381c2d1103",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "`tensorflow` is the main library for machine learning in Python.  \n",
    "\n",
    "`stock_prices.csv` is the data set that is loaded. \n",
    "\n",
    "`MinMaxScaler` method is used to normalize the data.  \n",
    "\n",
    "`create_dataset`method is used to prepare the data for training. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834533dc-d545-4225-abe5-bf6702a63b29",
   "metadata": {},
   "source": [
    "### Step 2: Implement Multi-Head Self-Attention \n",
    "\n",
    "Define the Multi-Head Self-Attention mechanism. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c17e005d-bb96-4e35-84f6-1b64ff95198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads=8): \n",
    "        super(MultiHeadSelfAttention, self).__init__() \n",
    "        self.embed_dim = embed_dim \n",
    "        self.num_heads = num_heads \n",
    "        self.projection_dim = embed_dim // num_heads \n",
    "        self.query_dense = Dense(embed_dim) \n",
    "        self.key_dense = Dense(embed_dim) \n",
    "        self.value_dense = Dense(embed_dim) \n",
    "        self.combine_heads = Dense(embed_dim) \n",
    "\n",
    "\n",
    "    def attention(self, query, key, value): \n",
    "        score = tf.matmul(query, key, transpose_b=True) \n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) \n",
    "        scaled_score = score / tf.math.sqrt(dim_key) \n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1) \n",
    "        output = tf.matmul(weights, value) \n",
    "        return output, weights \n",
    "\n",
    "    def split_heads(self, x, batch_size): \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim)) \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) \n",
    "\n",
    "    def call(self, inputs): \n",
    "        batch_size = tf.shape(inputs)[0] \n",
    "        query = self.query_dense(inputs) \n",
    "        key = self.key_dense(inputs) \n",
    "        value = self.value_dense(inputs) \n",
    "        query = self.split_heads(query, batch_size) \n",
    "        key = self.split_heads(key, batch_size) \n",
    "        value = self.split_heads(value, batch_size) \n",
    "        attention, _ = self.attention(query, key, value) \n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) \n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim)) \n",
    "        output = self.combine_heads(concat_attention) \n",
    "        return output \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9559b2f1-6474-4761-b3e1-e8d463be0b80",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The MultiHeadSelfAttention layer implements the multi-head self-attention mechanism, which allows the model to focus on different parts of the input sequence simultaneously. \n",
    "\n",
    "- The attention parameter computes the attention scores and weighted sum of the values. \n",
    "\n",
    "- The split_heads parameter splits the input into multiple heads for parallel attention computation. \n",
    "\n",
    "- The call method applies the self-attention mechanism and combines the heads. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eacfd8-cf47-49e2-b3d6-37f4a9f7fc91",
   "metadata": {},
   "source": [
    "### Step 3: Implement Transformer block \n",
    "\n",
    "Define the Transformer block. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d98b16c-1273-47db-a7c1-1b86c156f923",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerBlock, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ac1a9c-a6fd-426a-8150-7d73bbda0260",
   "metadata": {},
   "source": [
    "In the above code:\n",
    "\n",
    "- The TransformerBlock layer combines multi-head self-attention with a feed-forward neural network and normalization layers.  \n",
    "\n",
    "- Dropout is used to prevent overfitting. \n",
    "\n",
    "- The call method applies the self-attention, followed by the feedforward network with residual connections and layer normalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44236ed7-6e90-4272-8ddb-0b23f162e801",
   "metadata": {},
   "source": [
    "### Step 4: Implement Encoder Layer \n",
    "\n",
    "Define the Encoder layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ae62188-09fc-4efa-be57-4ccdc7388d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(EncoderLayer, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c4a011-d544-467e-8dd4-5b785a226924",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The EncoderLayer is similar to the TransformerBlock but is a reusable layer in the Transformer architecture. \n",
    "\n",
    "- It consists of a MultiHeadSelfAttention mechanism followed by a feedforward neural network. \n",
    "\n",
    "- Both sub-layers have residual connections around them, and layer normalization is applied to the output of each sub-layer. \n",
    "\n",
    "- The call method applies the self-attention, followed by the feedforward network, with residual connections and layer normalization. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d804e1-689b-4876-be82-6a65a1381154",
   "metadata": {},
   "source": [
    "### Step 5: Implement Transformer encoder \n",
    "\n",
    "Define the Transformer Encoder. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4cc36bd-6bd8-4334-8571-d3ec5a17c69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1735987790.607863 2807423 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9595 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:09:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 128)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout \n",
    "\n",
    "class MultiHeadSelfAttention(Layer): \n",
    "    def __init__(self, embed_dim, num_heads=8): \n",
    "        super(MultiHeadSelfAttention, self).__init__() \n",
    "        self.embed_dim = embed_dim \n",
    "        self.num_heads = num_heads \n",
    "        self.projection_dim = embed_dim // num_heads \n",
    "        self.query_dense = Dense(embed_dim) \n",
    "        self.key_dense = Dense(embed_dim) \n",
    "        self.value_dense = Dense(embed_dim) \n",
    "        self.combine_heads = Dense(embed_dim) \n",
    " \n",
    "\n",
    "    def attention(self, query, key, value): \n",
    "        score = tf.matmul(query, key, transpose_b=True) \n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) \n",
    "        scaled_score = score / tf.math.sqrt(dim_key) # normalize query-key score\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1) # readjust weights\n",
    "        output = tf.matmul(weights, value) # re-weight values\n",
    "        return output, weights \n",
    "\n",
    "    # the split_heads function is used to split the heads for the attention mechanism\n",
    "    # the input is reshaped from (batch_size, seq_len, embed_dim)\n",
    "    # to (batch_size, num_heads, seq_len, projection_dim)\n",
    "    def split_heads(self, x, batch_size): \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim)) \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) \n",
    "\n",
    "\n",
    "    def call(self, inputs): \n",
    "        batch_size = tf.shape(inputs)[0] \n",
    "        query = self.query_dense(inputs) \n",
    "        key = self.key_dense(inputs) \n",
    "        value = self.value_dense(inputs) \n",
    "        query = self.split_heads(query, batch_size) \n",
    "        key = self.split_heads(key, batch_size) \n",
    "        value = self.split_heads(value, batch_size) \n",
    "        attention, _ = self.attention(query, key, value) \n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) \n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim)) \n",
    "        output = self.combine_heads(concat_attention) \n",
    "        return output \n",
    "\n",
    "class TransformerBlock(Layer): \n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerBlock, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    " \n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) \n",
    "\n",
    "class TransformerEncoder(Layer): \n",
    "    def __init__(self, num_layers, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerEncoder, self).__init__() \n",
    "        self.num_layers = num_layers \n",
    "        self.embed_dim = embed_dim \n",
    "        self.enc_layers = [TransformerBlock(embed_dim, num_heads, ff_dim, rate) for _ in range(num_layers)] \n",
    "        self.dropout = Dropout(rate) \n",
    "\n",
    "    def call(self, inputs, training=False): \n",
    "        x = inputs \n",
    "        for i in range(self.num_layers): \n",
    "            x = self.enc_layers[i](x, training=training) \n",
    "        return x \n",
    "\n",
    "# Example usage \n",
    "embed_dim = 128 \n",
    "num_heads = 8 \n",
    "ff_dim = 512 \n",
    "num_layers = 4 \n",
    "\n",
    "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "inputs = tf.random.uniform((1, 100, embed_dim)) \n",
    "outputs = transformer_encoder(inputs, training=False)  # Use keyword argument for 'training' \n",
    "print(outputs.shape)  # Should print (1, 100, 128) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58914cf7-70fa-4e3a-9a13-2c7ea907f91e",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "The TransformerEncoder is composed of multiple TransformerBlock layers, implementing the encoding part of the Transformer architecture. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade3a268-1398-489b-965b-63d7cb4f70b9",
   "metadata": {},
   "source": [
    "### Step 6: Build and Compile the Transformer model \n",
    "\n",
    "Integrate the Transformer Encoder into a complete model for sequential data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "973dc690-4c2f-4edf-aa69-63be850f3ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">793,088</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12800</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,801</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m793,088\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12800\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │        \u001b[38;5;34m12,801\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the necessary parameters \n",
    "\n",
    "embed_dim = 128 \n",
    "num_heads = 8 \n",
    "ff_dim = 512 \n",
    "num_layers = 4 \n",
    "\n",
    "# Define the Transformer Encoder \n",
    "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "\n",
    "# Build the model \n",
    "input_shape = (X.shape[1], X.shape[2]) \n",
    "inputs = tf.keras.Input(shape=input_shape) \n",
    "\n",
    "# Project the inputs to the embed_dim \n",
    "x = tf.keras.layers.Dense(embed_dim)(inputs) \n",
    "encoder_outputs = transformer_encoder(x) \n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "outputs = tf.keras.layers.Dense(1)(flatten) \n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "# Compile the model \n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "\n",
    "# Summary of the model \n",
    "model.summary() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fb9cf5-9372-4794-add8-5f2392838a23",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The Transformer Encoder model defines the necessary parameters, flattens the output, and ends with a dense layer to produce the final output.  \n",
    "\n",
    "- The model is then compiled with the Adam optimizer and mean squared error loss. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5978fb3-3a42-44f9-b146-68cad41ba794",
   "metadata": {},
   "source": [
    "### Step 7: Train the Transformer model \n",
    "\n",
    "Train the model on the prepared dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a65d7244-5b68-4ba7-9f94-27cb0022e768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1735987797.640264 2807708 service.cc:148] XLA service 0x722ff0002ed0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1735987797.640293 2807708 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "2025-01-04 11:49:57.782017: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1735987798.436519 2807708 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-01-04 11:49:59.501374: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 188 bytes spill stores, 188 bytes spill loads\n",
      "\n",
      "2025-01-04 11:49:59.998606: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_99', 620 bytes spill stores, 572 bytes spill loads\n",
      "\n",
      "2025-01-04 11:50:00.298787: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6_0', 768 bytes spill stores, 720 bytes spill loads\n",
      "\n",
      "2025-01-04 11:50:00.401832: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_93', 196 bytes spill stores, 196 bytes spill loads\n",
      "\n",
      "2025-01-04 11:50:00.880668: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6_0', 228 bytes spill stores, 228 bytes spill loads\n",
      "\n",
      "2025-01-04 11:50:01.012816: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 36 bytes spill stores, 36 bytes spill loads\n",
      "\n",
      "2025-01-04 11:50:01.250385: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8614', 60 bytes spill stores, 88 bytes spill loads\n",
      "\n",
      "2025-01-04 11:50:01.357121: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8614', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-01-04 11:50:01.373612: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_93', 68 bytes spill stores, 72 bytes spill loads\n",
      "\n",
      "2025-01-04 11:50:01.382753: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 388 bytes spill stores, 356 bytes spill loads\n",
      "\n",
      "2025-01-04 11:50:01.494501: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 592 bytes spill stores, 552 bytes spill loads\n",
      "\n",
      "2025-01-04 11:50:01.600190: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "2025-01-04 11:50:01.693853: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 36 bytes spill stores, 36 bytes spill loads\n",
      "\n",
      "2025-01-04 11:50:01.829964: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8561', 24 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-01-04 11:50:01.969065: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 20 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "2025-01-04 11:50:02.001496: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8561', 40 bytes spill stores, 40 bytes spill loads\n",
      "\n",
      "2025-01-04 11:50:02.212840: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2025-01-04 11:50:02.551419: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8614', 28 bytes spill stores, 28 bytes spill loads\n",
      "\n",
      "2025-01-04 11:50:02.604040: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8614', 76 bytes spill stores, 76 bytes spill loads\n",
      "\n",
      "2025-01-04 11:50:02.608095: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 56 bytes spill stores, 76 bytes spill loads\n",
      "\n",
      "2025-01-04 11:50:02.696101: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8561', 44 bytes spill stores, 44 bytes spill loads\n",
      "\n",
      "2025-01-04 11:50:02.728047: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 560 bytes spill stores, 528 bytes spill loads\n",
      "\n",
      "2025-01-04 11:50:02.755336: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8614', 92 bytes spill stores, 92 bytes spill loads\n",
      "\n",
      "2025-01-04 11:50:03.036349: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8614', 748 bytes spill stores, 736 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/60\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 31.8608"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1735987807.421525 2807708 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m57/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 14.8932"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-04 11:50:09.628029: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_99', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-01-04 11:50:10.078137: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_99', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2025-01-04 11:50:10.176937: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 204 bytes spill stores, 204 bytes spill loads\n",
      "\n",
      "2025-01-04 11:50:10.193922: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_93', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-01-04 11:50:10.549497: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_27', 60 bytes spill stores, 60 bytes spill loads\n",
      "\n",
      "2025-01-04 11:50:10.727241: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_27', 64 bytes spill stores, 72 bytes spill loads\n",
      "\n",
      "2025-01-04 11:50:10.796081: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6_0', 756 bytes spill stores, 444 bytes spill loads\n",
      "\n",
      "2025-01-04 11:50:11.555209: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "2025-01-04 11:50:11.704568: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_93', 80 bytes spill stores, 80 bytes spill loads\n",
      "\n",
      "2025-01-04 11:50:11.815635: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 64 bytes spill stores, 72 bytes spill loads\n",
      "\n",
      "2025-01-04 11:50:11.894351: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8614_0', 132 bytes spill stores, 132 bytes spill loads\n",
      "\n",
      "2025-01-04 11:50:12.400058: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 60 bytes spill stores, 60 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 155ms/step - loss: 14.2291\n",
      "Epoch 2/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.2174\n",
      "Epoch 3/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.1603\n",
      "Epoch 4/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.1337\n",
      "Epoch 5/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.1681\n",
      "Epoch 6/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.1911\n",
      "Epoch 7/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.1147\n",
      "Epoch 8/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.1556\n",
      "Epoch 9/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.1344\n",
      "Epoch 10/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.1106\n",
      "Epoch 11/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.1196\n",
      "Epoch 12/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.1216\n",
      "Epoch 13/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.1161\n",
      "Epoch 14/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0799\n",
      "Epoch 15/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0910\n",
      "Epoch 16/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0709\n",
      "Epoch 17/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.1107\n",
      "Epoch 18/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.1136\n",
      "Epoch 19/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0743\n",
      "Epoch 20/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0678\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x72318cba4440>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa04758-fc9b-4fb2-b00e-535326f274a6",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "The model is trained on the normalized stock price data for 20 epochs with a batch size of 32. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c73638-a697-414f-91b4-3cf1455015db",
   "metadata": {},
   "source": [
    "### Step 8: Evaluate and Make Predictions \n",
    "\n",
    "Evaluate the model's performance and make predictions on the dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14dc0b3a-758e-407e-b392-cd3b9a1c2fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions \n",
    "predictions = model.predict(X) \n",
    "predictions = scaler.inverse_transform(predictions) \n",
    " \n",
    "\n",
    "# Plot the predictions \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(df[['Close']].values , label='True Data') \n",
    "plt.plot(np.arange(time_step, time_step + len(predictions)), predictions, label='Predictions') \n",
    "plt.xlabel('Time') \n",
    "plt.ylabel('Stock Price') \n",
    "plt.legend() \n",
    "plt.show() \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7654ac42-8a22-4304-b257-2d344dcd360f",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The model's predictions are transformed back to the original scale using the inverse transform of the scaler. \n",
    "\n",
    "- The true data and predictions are plotted to visualize the model's performance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9e038e-2bd0-48df-a073-317143f34a65",
   "metadata": {},
   "source": [
    "## Practice Exercises: \n",
    "\n",
    " ### Exercise 1: Add dropout to the Transformer model \n",
    "\n",
    " **Objective: Understand how to add dropout layers to the Transformer model to prevent overfitting.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Add a dropout layer after the Flatten layer in the model. \n",
    "\n",
    "- Set the dropout rate to 0.5. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "658814d0-81f8-4e42-9196-f58a8ba73174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_13\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_13\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_74 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">793,088</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12800</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12800</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_75 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,801</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_9 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_74 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m793,088\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12800\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_27 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12800\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_75 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │        \u001b[38;5;34m12,801\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 100ms/step - loss: 13.6034\n",
      "Epoch 2/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 2.1703\n",
      "Epoch 3/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 1.4670\n",
      "Epoch 4/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.9846\n",
      "Epoch 5/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.3841\n",
      "Epoch 6/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.1694\n",
      "Epoch 7/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0626\n",
      "Epoch 8/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0415\n",
      "Epoch 9/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0410\n",
      "Epoch 10/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0336\n",
      "Epoch 11/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0272\n",
      "Epoch 12/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0224\n",
      "Epoch 13/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0241\n",
      "Epoch 14/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0231\n",
      "Epoch 15/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0192\n",
      "Epoch 16/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0249\n",
      "Epoch 17/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0195\n",
      "Epoch 18/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0188\n",
      "Epoch 19/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0209\n",
      "Epoch 20/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0161\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.0036\n",
      "Time taken to train the model: 28.780834436416626\n",
      "Loss on the training set: 0.005455803591758013\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Define the necessary parameters \n",
    "\n",
    "embed_dim = 128 \n",
    "num_heads = 8 \n",
    "ff_dim = 512 \n",
    "num_layers = 4 \n",
    "\n",
    "# Define the Transformer Encoder \n",
    "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "\n",
    "# Build the model \n",
    "input_shape = (X.shape[1], X.shape[2]) \n",
    "inputs = tf.keras.Input(shape=input_shape) \n",
    "\n",
    "# Project the inputs to the embed_dim \n",
    "x = tf.keras.layers.Dense(embed_dim)(inputs) \n",
    "encoder_outputs = transformer_encoder(x) \n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs)\n",
    "drop = tf.keras.layers.Dropout(0.5)(flatten) \n",
    "outputs = tf.keras.layers.Dense(1)(drop) \n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "# Compile the model \n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary() \n",
    "\n",
    "# Train the model\n",
    "start = time.time()\n",
    "historydrop = model.fit(X, Y, epochs=20, batch_size=32)\n",
    "end = time.time()\n",
    "loss = model.evaluate(X, Y)\n",
    "\n",
    "print(\"Time taken to train the model:\", end-start)\n",
    "print(\"Loss on the training set:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b631728-90c3-4045-9a46-4981381b9f26",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "from tensorflow.keras.layers import Dropout \n",
    "\n",
    "  \n",
    "\n",
    "# Add a dropout layer after the Flatten layer \n",
    "\n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "\n",
    "dropout = Dropout(0.5)(flatten) \n",
    "\n",
    "outputs = tf.keras.layers.Dense(1)(dropout) \n",
    "\n",
    "  \n",
    "\n",
    "# Build the model \n",
    "\n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "  \n",
    "\n",
    "# Compile the model \n",
    "\n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "\n",
    "  \n",
    "\n",
    "# Train the model \n",
    "\n",
    "model.fit(X, Y, epochs=20, batch_size=32) \n",
    "\n",
    "  \n",
    "\n",
    "# Evaluate the model \n",
    "\n",
    "loss = model.evaluate(X, Y) \n",
    "\n",
    "print(f'Test loss: {loss}') \n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac4285e-8886-47e6-8946-e10493394914",
   "metadata": {},
   "source": [
    "### Exercise 2: Experiment with different batch sizes \n",
    "\n",
    "**Objective: Observe the impact of different batch sizes on model performance.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Train the model with a batch size of 16. \n",
    "\n",
    "- Train the model with a batch size of 64. \n",
    "\n",
    "- Compare the training time and performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_18\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_18\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_100 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">793,088</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12800</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_101 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,801</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_14 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_100 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m793,088\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12800\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_101 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │        \u001b[38;5;34m12,801\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-04 11:51:10.639118: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_99', 48 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "2025-01-04 11:51:10.804127: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_93_0', 72 bytes spill stores, 72 bytes spill loads\n",
      "\n",
      "2025-01-04 11:51:10.825650: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_99_0', 56 bytes spill stores, 56 bytes spill loads\n",
      "\n",
      "2025-01-04 11:51:11.041285: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_93', 80 bytes spill stores, 84 bytes spill loads\n",
      "\n",
      "2025-01-04 11:51:11.404744: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 188 bytes spill stores, 188 bytes spill loads\n",
      "\n",
      "2025-01-04 11:51:11.457623: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6_0', 756 bytes spill stores, 444 bytes spill loads\n",
      "\n",
      "2025-01-04 11:51:11.501863: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_27', 96 bytes spill stores, 96 bytes spill loads\n",
      "\n",
      "2025-01-04 11:51:11.699530: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 96 bytes spill stores, 96 bytes spill loads\n",
      "\n",
      "2025-01-04 11:51:12.023551: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59_0', 96 bytes spill stores, 96 bytes spill loads\n",
      "\n",
      "2025-01-04 11:51:12.078815: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25_0', 56 bytes spill stores, 56 bytes spill loads\n",
      "\n",
      "2025-01-04 11:51:12.484104: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_93', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-01-04 11:51:12.521628: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57_0', 96 bytes spill stores, 96 bytes spill loads\n",
      "\n",
      "2025-01-04 11:51:12.825091: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "2025-01-04 11:51:13.240064: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8614_0', 132 bytes spill stores, 132 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 49ms/step - loss: 9.3323\n",
      "Epoch 2/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.2817\n",
      "Epoch 3/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1934\n",
      "Epoch 4/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1688\n",
      "Epoch 5/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1547\n",
      "Epoch 6/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.1325\n",
      "Epoch 7/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1427\n",
      "Epoch 8/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1709\n",
      "Epoch 9/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.1221\n",
      "Epoch 10/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0777\n",
      "Epoch 11/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0674\n",
      "Epoch 12/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0443\n",
      "Epoch 13/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0402\n",
      "Epoch 14/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0324\n",
      "Epoch 15/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0209\n",
      "Epoch 16/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0189\n",
      "Epoch 17/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0271\n",
      "Epoch 18/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0157\n",
      "Epoch 19/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0133\n",
      "Epoch 20/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0094\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - loss: 0.0063\n",
      "Training time with batch_size=16: 35.76 seconds\n",
      "Loss with batch_size=16: 0.008186821825802326\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Define the necessary parameters \n",
    "embed_dim = 128 \n",
    "num_heads = 8 \n",
    "ff_dim = 512 \n",
    "num_layers = 4 \n",
    "\n",
    "# Define the Transformer Encoder \n",
    "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "\n",
    "# Build the model \n",
    "input_shape = (X.shape[1], X.shape[2]) \n",
    "inputs = tf.keras.Input(shape=input_shape) \n",
    "\n",
    "# Project the inputs to the embed_dim \n",
    "x = tf.keras.layers.Dense(embed_dim)(inputs) \n",
    "encoder_outputs = transformer_encoder(x) \n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs)\n",
    "outputs = tf.keras.layers.Dense(1)(flatten) \n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "# Compile the model \n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary() \n",
    "\n",
    "# Train with batch size 16\n",
    "start_time = time.time()\n",
    "history16 = model.fit(X, Y, epochs=20, batch_size=16)\n",
    "training_time = time.time() - start_time\n",
    "loss = model.evaluate(X, Y)\n",
    "\n",
    "print(f\"Training time with batch_size=16: {training_time:.2f} seconds\")\n",
    "print(f\"Loss with batch_size=16: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "960017cb-8c0e-4d60-9447-81f4be936add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_23\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_23\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_126 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">793,088</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12800</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_127 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,801</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_19 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_126 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m793,088\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12800\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_127 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │        \u001b[38;5;34m12,801\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-04 11:51:48.954310: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 188 bytes spill stores, 188 bytes spill loads\n",
      "\n",
      "2025-01-04 11:51:49.104990: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 388 bytes spill stores, 356 bytes spill loads\n",
      "\n",
      "2025-01-04 11:51:49.247845: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_99', 620 bytes spill stores, 572 bytes spill loads\n",
      "\n",
      "2025-01-04 11:51:50.444651: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6_0', 1720 bytes spill stores, 2280 bytes spill loads\n",
      "\n",
      "2025-01-04 11:51:50.615767: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6_0', 228 bytes spill stores, 228 bytes spill loads\n",
      "\n",
      "2025-01-04 11:51:51.118850: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8561_0', 132 bytes spill stores, 132 bytes spill loads\n",
      "\n",
      "2025-01-04 11:51:51.277083: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_93', 196 bytes spill stores, 196 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 24.8124"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-04 11:51:57.942828: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_99', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2025-01-04 11:51:58.091081: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_99', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-01-04 11:51:58.356682: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6_0', 756 bytes spill stores, 444 bytes spill loads\n",
      "\n",
      "2025-01-04 11:51:58.370078: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_99', 612 bytes spill stores, 436 bytes spill loads\n",
      "\n",
      "2025-01-04 11:51:58.381988: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 384 bytes spill stores, 352 bytes spill loads\n",
      "\n",
      "2025-01-04 11:51:58.773213: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 204 bytes spill stores, 204 bytes spill loads\n",
      "\n",
      "2025-01-04 11:51:59.169088: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_93', 80 bytes spill stores, 80 bytes spill loads\n",
      "\n",
      "2025-01-04 11:51:59.170503: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "2025-01-04 11:51:59.290498: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 60 bytes spill stores, 60 bytes spill loads\n",
      "\n",
      "2025-01-04 11:51:59.641425: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_93', 208 bytes spill stores, 204 bytes spill loads\n",
      "\n",
      "2025-01-04 11:51:59.769114: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_93', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-01-04 11:51:59.930227: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_27', 64 bytes spill stores, 72 bytes spill loads\n",
      "\n",
      "2025-01-04 11:51:59.969024: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 64 bytes spill stores, 72 bytes spill loads\n",
      "\n",
      "2025-01-04 11:52:00.013000: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8561_0', 132 bytes spill stores, 132 bytes spill loads\n",
      "\n",
      "2025-01-04 11:52:00.782713: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_27', 60 bytes spill stores, 60 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 315ms/step - loss: 23.8627\n",
      "Epoch 2/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.4183\n",
      "Epoch 3/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.2031\n",
      "Epoch 4/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1740\n",
      "Epoch 5/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1680\n",
      "Epoch 6/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1252\n",
      "Epoch 7/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.1333\n",
      "Epoch 8/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1204\n",
      "Epoch 9/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1568\n",
      "Epoch 10/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1254\n",
      "Epoch 11/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1330\n",
      "Epoch 12/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1126\n",
      "Epoch 13/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1590\n",
      "Epoch 14/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1496\n",
      "Epoch 15/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1357\n",
      "Epoch 16/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1055\n",
      "Epoch 17/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.1018\n",
      "Epoch 18/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1324\n",
      "Epoch 19/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1344\n",
      "Epoch 20/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0936\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0079\n",
      "Training time with batch_size=64: 33.31 seconds\n",
      "Loss with batch_size=64: 0.0043982635252177715\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Define the necessary parameters \n",
    "embed_dim = 128 \n",
    "num_heads = 8 \n",
    "ff_dim = 512 \n",
    "num_layers = 4 \n",
    "\n",
    "# Define the Transformer Encoder \n",
    "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "\n",
    "# Build the model \n",
    "input_shape = (X.shape[1], X.shape[2]) \n",
    "inputs = tf.keras.Input(shape=input_shape) \n",
    "\n",
    "# Project the inputs to the embed_dim \n",
    "x = tf.keras.layers.Dense(embed_dim)(inputs) \n",
    "encoder_outputs = transformer_encoder(x) \n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs)\n",
    "outputs = tf.keras.layers.Dense(1)(flatten) \n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "# Compile the model \n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary() \n",
    "\n",
    "# Train with batch size 64\n",
    "start_time = time.time()\n",
    "history64 = model.fit(X, Y, epochs=20, batch_size=64)\n",
    "training_time = time.time() - start_time\n",
    "loss = model.evaluate(X, Y)\n",
    "\n",
    "print(f\"Training time with batch_size=64: {training_time:.2f} seconds\")\n",
    "print(f\"Loss with batch_size=64: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f752d7-fece-4267-9164-9a64bcae3911",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "# Train the model with batch size 16\n",
    "model.fit(X, Y, epochs=20, batch_size=16)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 16: {loss}')\n",
    "\n",
    "# Train the model with batch size 64\n",
    "model.fit(X, Y, epochs=20, batch_size=64)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 64: {loss}')\n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359aec66-7a1c-4f1b-9be3-6a7f90905c3d",
   "metadata": {},
   "source": [
    "### Exercise 3: Use a different activation function \n",
    "\n",
    " **Objective: Understand how different activation functions impact the model performance.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Change the activation function of the Dense layer to `tanh`. \n",
    "\n",
    "- Train and evaluate the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "194078fc-6b2a-4543-8200-b3d6c9e71e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_28\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_28\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_152 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">793,088</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12800</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_153 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,801</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_24 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_152 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m793,088\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_4 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12800\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_153 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │        \u001b[38;5;34m12,801\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 95ms/step - loss: 0.4587\n",
      "Epoch 2/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.2975\n",
      "Epoch 3/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.2960\n",
      "Epoch 4/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.3033\n",
      "Epoch 5/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.2939\n",
      "Epoch 6/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.3038\n",
      "Epoch 7/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.3058\n",
      "Epoch 8/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.2946\n",
      "Epoch 9/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.3008\n",
      "Epoch 10/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.2863\n",
      "Epoch 11/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.3041\n",
      "Epoch 12/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.2978\n",
      "Epoch 13/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.3078\n",
      "Epoch 14/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.3032\n",
      "Epoch 15/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.2926\n",
      "Epoch 16/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.2997\n",
      "Epoch 17/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.3019\n",
      "Epoch 18/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.2952\n",
      "Epoch 19/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.2874\n",
      "Epoch 20/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.3011\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.5205\n",
      "Training time: 28.63 seconds\n",
      "Loss with tanh activation: 0.29668867588043213\n"
     ]
    }
   ],
   "source": [
    "# Define the necessary parameters \n",
    "\n",
    "embed_dim = 128 \n",
    "num_heads = 8 \n",
    "ff_dim = 512 \n",
    "num_layers = 4 \n",
    "\n",
    "# Define the Transformer Encoder \n",
    "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "\n",
    "# Build the model \n",
    "input_shape = (X.shape[1], X.shape[2]) \n",
    "inputs = tf.keras.Input(shape=input_shape) \n",
    "\n",
    "# Project the inputs to the embed_dim \n",
    "x = tf.keras.layers.Dense(embed_dim)(inputs) \n",
    "encoder_outputs = transformer_encoder(x) \n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs)\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"tanh\")(flatten) \n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "# Compile the model \n",
    "model.compile(optimizer=\"adam\", loss=\"mse\") \n",
    "\n",
    "# Summary of the model \n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "historytanh = model.fit(X, Y, epochs=20, batch_size=32)\n",
    "training_time = time.time() - start_time\n",
    "loss = model.evaluate(X, Y)\n",
    "\n",
    "print(f\"Training time: {training_time:.2f} seconds\")\n",
    "print(f\"Loss with tanh activation: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade7ca29-e106-4a76-97b6-5fd814a01a0e",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "# Change the activation function of the Dense layer to tanh\n",
    "outputs = tf.keras.layers.Dense(1, activation='tanh')(flatten)\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with tanh activation: {loss}')\n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b057d6-dab5-465e-bfc2-a2e1238297ad",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Congratulations on completing this lab! In this lab, you have built an advanced Transformer model using Keras and applied it to a time series forecasting task. You have learned how to define and implement multi-head self-attention, Transformer blocks, encoder layers, and integrate them into a complete Transformer model. By experimenting with different configurations and training the model, you can further improve its performance and apply it to various sequential data tasks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d130ec79-66d4-4f13-9e97-f5d0508ec412",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "prev_pub_hash": "28ac4fd81c1d713f83dcd1cdf1d3383ad25ea92873288fe9e978e9a17b314709"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
