{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "\n",
    "## Convolution\n",
    "* transformation of images through kernel to activation map\n",
    "* kernel window, multiply image intensity by a kernel, sum the results, creates element in the activation map, slide window\n",
    "* the size of the activation map can be computed as follows>\n",
    "    * let $M$ be size of the image, $p$ stands for size of padding, $K$ size of the kernel, and $S$ stride (step size)\n",
    "    * vertical/horizontal number of steps is then $\\frac{M+2p-K}{S}+1$\n",
    "* padding\n",
    "    * adding zeros around the image allowing for convolution around the img borders\n",
    "\n",
    "## Activations and pooling\n",
    "* activation function is applied element-wise on result of the convolution operation\n",
    "* pooling reduces size of the activation map, not channels, but vertical and horizontal size\n",
    "\n",
    "## Channels\n",
    "* different kernels detect different features within a image\n",
    "* let $l$ denotes the input channel and $k$ denotes the input index\n",
    "* then the convolution operation is defined as $(Z)_l = \\sum_{k=1}^{K} W_{l,k} * (X)_k + b_l$\n",
    "\n",
    "## CNN\n",
    "* typical architecture involves convolution blocks (conv+pooling), and dense head (flatten>dense>output layer)\n",
    "* forward pass is similar to ffn (conceptually), flattening is handled through view(-1)\n",
    "* backprop is also similar to ffn (conceptually)\n",
    "* size of final dense layer\n",
    "    * input -> output channels * size after last pooling (?)\n",
    "    * output -> number of classes\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
