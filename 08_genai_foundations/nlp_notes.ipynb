{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of language understanding\n",
    "\n",
    "### Converting word to features\n",
    "* one-hot encoding of tokens, bag of words (order omitted)\n",
    "* token index & embedding layer (replacing token index with embedded vector)\n",
    "* embedding matrix - tokens (rows) vs embedding dimension (cols)\n",
    "* multiple words -> sum/avg of the embeddings (embedding bag)\n",
    "\n",
    "### N-grams\n",
    "* context provided by close words\n",
    "* might be used with conditional probability for a next word prediction\n",
    "* feed-forward neural net approach\n",
    "    * neural network can approximate the conditional probability for arbitrary context size\n",
    "    * input layer -> context size x vocab size (embedding dim)\n",
    "    * output layer -> vocab size (the next word)\n",
    "    * such representation does not take order within the context into account\n",
    "\n",
    "\n",
    "### W2V\n",
    "* producing numerical representation of words (embeddings)\n",
    "* Context bag-of-words\n",
    "    * architecture\n",
    "        * input layer - context * vocab size\n",
    "        * embedding layer (hidden) - embedding size\n",
    "        * output layer - vocab size & softmax activation \n",
    "    * data\n",
    "        * context -> before & after word, or the next word\n",
    "* Skip-gram\n",
    "        * predicts surrounding context, input & output layers switched\n",
    "\n",
    "### Seq2Seq & RNN\n",
    "* multiple inputs -> multiple outputs, no fixed/equal length!\n",
    "* internal memory to propagate internal state across time steps\n",
    "* RNN, GRU, LSTM\n",
    "\n",
    "### Translation nets\n",
    "* Seq2Seq\n",
    "    * cross-entropy loss on target labels\n",
    "    * init training mode, activate layers\n",
    "    * iterate through train, push src and trg sequences to correct devices\n",
    "    * generate predictions\n",
    "    * reshape the output to be aligned with loss calc\n",
    "    * calc average loss\n",
    "* enc-dec RNN\n",
    "    * encoder RNNs (src)\n",
    "        * input -> token, hidden state\n",
    "        * embeddings\n",
    "        * hidden state\n",
    "        * output -> token, hidden state to the next cell\n",
    "    * last hidden state passed to decoder\n",
    "    * decoder RNNs (passed)\n",
    "        * input -> token (previously generated), hidden state\n",
    "        * embeddings\n",
    "        * output -> token, hidden state ti the next cell\n",
    "    \n",
    "### Evaluation metrics\n",
    "* perplexity\n",
    "    * $L = -\\frac{1}{N} \\sum_{i=1}^N P(w_i) log (Q(w_i))$\n",
    "    * $PPL = e^L$\n",
    "    * how model learned the training set\n",
    "* precision & recall\n",
    "    * $pre = \\frac{count_{matched}}{count_{generated}}$\n",
    "        * accuracy\n",
    "    * $rec = \\frac{count_{matched}}{count_{reference}}$\n",
    "        * coverage\n",
    "    * f1 balances both of perspectives\n",
    "* others\n",
    "    * bleu, meteor, rouge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ethical Implications of Word Embeddings and Language Models\n",
    "\n",
    "With the increasing use of Word2Vec and sequence-to-sequence models, itâ€™s crucial to consider the ethical implications of language model training data. Here, we explore key ethical considerations for Word2Vec embeddings and sequence-to-sequence models, focusing on **bias**, **privacy**, and **fair representation**.\n",
    "\n",
    "### Bias in Word Embeddings\n",
    "\n",
    "Word embeddings, such as those created with Word2Vec, can inadvertently capture and amplify biases present in the training data. For example, associations between gendered terms and certain professions (like \"doctor\" with \"male\" and \"nurse\" with \"female\") can reflect societal biases. When these models are applied, these biases may be transferred into automated processes and decisions, impacting real-world outcomes. Some ways to mitigate bias include:\n",
    "\n",
    "- **Debiasing techniques**: Algorithms that detect and reduce biased associations are actively being developed. One approach is to neutralize gender associations by re-centering biased vectors or applying \"debiasing\" during training.\n",
    "- **Evaluation for fairness**: Regularly evaluating models for bias during development can help identify areas of concern early, ensuring better alignment with ethical standards.\n",
    "\n",
    "### Privacy and Data Usage\n",
    "\n",
    "Large datasets used to train language models often include a broad range of data, sometimes containing sensitive or private information. Models trained on such data might inadvertently memorize personal information or sensitive details, which could be exposed in generated responses. Protecting privacy during model training involves:\n",
    "\n",
    "- **Data anonymization**: Ensuring datasets are anonymized to remove identifiable information minimizes the risk of exposing private data.\n",
    "- **Differential privacy**: Advanced techniques like differential privacy allow models to learn patterns without retaining specific details about any single individual in the dataset.\n",
    "- **Consent and transparency**: Ideally, datasets should be collected with informed consent from participants, ensuring users are aware of how their data is being used.\n",
    "\n",
    "### Fair Representation\n",
    "\n",
    "Language models need to be trained on data that represents a broad spectrum of demographics, languages, and cultural nuances to perform well across varied user groups. Without fair representation, models may underperform for underrepresented groups, leading to unintended consequences such as biased outputs or inaccurate translations.\n",
    "\n",
    "- **Demographic diversity in training data**: Collecting data from diverse sources ensures a balanced model that works well for various user demographics.\n",
    "- **Inclusive evaluation metrics**: Using metrics that assess model performance across demographic groups allows developers to measure inclusivity and identify areas for improvement.\n",
    "- **Continuous monitoring and updates**: As social norms and language usage evolve, regularly updating models and datasets ensures that they remain relevant, minimizing biases that may become apparent over time.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "These ethical considerations remind us of the importance of transparency and accountability in the development of language models. By prioritizing ethical data handling and model evaluation practices, we can reduce unintended biases, respect privacy, and build models that foster greater trust and inclusivity in AI applications. As Word2Vec and sequence-to-sequence models continue to influence NLP advancements, thoughtful attention to these areas will support more responsible and equitable technology."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
