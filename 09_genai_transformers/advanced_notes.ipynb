{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced concepts of transformer architecture\n",
    "\n",
    "## Decoder models\n",
    "\n",
    "* transforsmers were developed for language translation, they usually consist of two parts\n",
    "    * encoder -> process input data (source language sentence)\n",
    "    * decoder -> generates output data (translated sentence)\n",
    "* over time, decodes have become central to text generation tasks, forming basis of models like GPT, LLaMA\n",
    "\n",
    "* decoder model\n",
    "    * generative pre-training - > model is trained to predict the next word in a sequence based on previous words\n",
    "    * autoregressive models -> sequentially generating text, predicting text on words that came before\n",
    "\n",
    "* fine-tuning & reinforcement learning\n",
    "    * after initial training, GPT models can be fine-tuned for specific tasks (question answering or classification), model is trained on labeled data (supervised learning)\n",
    "    * reinforcement learning from human feedback is a fine-tuning method where human feedback is used to improve the performance, especially in apps like chatbots\n",
    "\n",
    "* decoders in text generation\n",
    "    * key difference from translation is that decoders rely on input from encoders (using cross-attention), in text generation they are independent, predicting next word based on preciding sequence\n",
    "    * autoregressive process starts with begining-of-sentence token, it predicts the next word, and appends it to the sequence.\n",
    "\n",
    "* masked self-attention in decoders hiddes future tokens\n",
    "\n",
    "* text generation process\n",
    "    * input prompt\n",
    "    * tokenization\n",
    "    * word embeddings\n",
    "    * positional encoding\n",
    "    * contextual embeddings\n",
    "    * logits\n",
    "    * argmax\n",
    "    * appending & repeat\n",
    "    * generation stops once end-of-sequence token or token limit is hit\n",
    "\n",
    "### Decoder training\n",
    "\n",
    "* key concepts\n",
    "    * autoregressive approach -> the text is generated sequentially, based on previously generated tokens\n",
    "    * causal attention masking -> future tokens are hidden to prevent leakage\n",
    "\n",
    "* notation\n",
    "    * $\\Omega_t$ stands for the predicted token at time step $t$, obtained through decoders final layer\n",
    "    * $\\hat x_t$ represents predicted word embedding at time step $t$, the hat indicates it is an estimate\n",
    "    * positional encoding is added to word embeddings to provide information about the position\n",
    "\n",
    "* autoregressive prediction process\n",
    "    * start with the first word embedding $x_0$ at time $t=0$\n",
    "    * the decoder generates contextual embeddings\n",
    "    * contextual embeddings are fed to the net predicting the next word\n",
    "    * the predicted word embedding is combined with original word embedding and fed back to the model\n",
    "    * process repeats until end-of-sequence token or limit for tokens is reached\n",
    "\n",
    "* training process\n",
    "    * training data -> input sentences are paired with output (shifted sequences), special tokens might be needed (bos, eos)\n",
    "    * word embeddings -> input tokens are converted into word embeddings and fed into the model\n",
    "    * causal masking -> prevents the model to see the future tokens, negative infinity is applied to the upper triangle of the attention mask, forcing probability to zero after softmax\n",
    "    * teacher forcing -> model is fed actual, not predicted previous token, might help with training (model is aligned with the actual sequence)\n",
    "    * loss function -> comparing predicted and actual tokens, calculating for every position in the sequence\n",
    "\n",
    "* training vs inference\n",
    "    * in training model processes the entire input sequence at once, uses actual word embeddings for training, employs teacher forcing to ensure correct inputs,\n",
    "    * inference is autoregressive, predictions depend on previously generated tokens\n",
    "\n",
    "### Causal Language model in Pytorch\n",
    "\n",
    "* causal language model predicts the next word in a sequence based on the previous words\n",
    "* causal masking as part of the training process\n",
    "\n",
    "* dataset\n",
    "    * IMDB reviews -> texts & sentiments\n",
    "    * special tokens include UNK, PAD, EOS\n",
    "\n",
    "* processing\n",
    "    * context size -> how many tokens serve as an input for predicting the next token\n",
    "    * select a point in the sequence of equal length to the block size\n",
    "    * create target sequence by shifting the previous sequence\n",
    "    * collate function to combine multiple sequences into batch & pad them for fixed size\n",
    "\n",
    "* masking\n",
    "    * imputing negative inf to upper triangular part of attention matrix\n",
    "    ```\n",
    "    [0, -inf, -inf, -inf]\n",
    "    [0,   0, -inf, -inf]\n",
    "    [0,   0,    0, -inf]\n",
    "    [0,   0,    0,    0]\n",
    "    ```\n",
    "\n",
    "* architecture\n",
    "    * embedding layer -> to map indexed tokens to their higher-dim representation\n",
    "    * positional encoding -> adds information about position of each token in the sentence\n",
    "    * transformer decoder -> multiple self-attention heads, uses causal mask\n",
    "    * linear layer -> outputs logit over the vocabulary size\n",
    "    * forward pass utilizes all of the steps in sequential layer\n",
    "\n",
    "* training\n",
    "    * input sequence, target sequence obtained\n",
    "    * loss computed with cross-entropy across target sequence\n",
    "    * backward-pass for param update\n",
    "\n",
    "## Encoder models\n",
    "\n",
    "* BERT (bidirectional encoder representations from transformers), it is pre-trained in self-supervised manner on large corpora and can be fine-tuned to specific tasks, pre-training usually using masked language modeling or next sentence prediction\n",
    "\n",
    "* architecture\n",
    "    * encoder-only approach, thus cannot be used for text generation, excels in comprehension though\n",
    "    * process texts in both direction (left and right)\n",
    "\n",
    "* key features\n",
    "    * bidirectional context -> looking at tokens before and after given word to understand meaning/predict the missing word\n",
    "    * segment embeddings -> segment embeddings to distinguish two sentences in paired tasks (question-answering)\n",
    "    * positional encodings -> same as in traditional transformer architecture\n",
    "\n",
    "* masked language modeling (MLM)\n",
    "    * randomly masking some words in a sentence and training BERT to predict them\n",
    "    * helps to learn contextual representation of words and understanding their relationships\n",
    "    * masking strategy\n",
    "        * replace 15 % of tokens\n",
    "        * 80 % of them as [MASK]\n",
    "        * 10 % of them with a random word\n",
    "        * 10 % of them unchanged\n",
    "    * prediction\n",
    "        * processes the entire sequence and generates contextual embeddings\n",
    "        * embeddings passed through linear layer to produce logits, predicting a word with highest prob\n",
    "\n",
    "* next sentence prediction (NSP)\n",
    "    * determines whether one sentence logically follows another, which is useful for question answering\n",
    "    * process overview\n",
    "        * two sentences are combined into a single sequence with special tokens denoting start and end of sequences (CLS, SEP), to separate them\n",
    "        * segment embeddings are used to indicate whether a token belongs to the first or second sentence\n",
    "    * strategy\n",
    "        * model predicts whether the second sentence is the actual next sentence or a random sentence\n",
    "        * a binary label is used\n",
    "\n",
    "* training -> minimizing the combined loss from both MLM and NSP tasks\n",
    "* fine-tuning -> adjusting the model to specific tasks such as sentiment analysis, text summarization or Q&A\n",
    "* key differences between encoder and decoder architectures\n",
    "    * encoder\n",
    "        * process entire sequence\n",
    "        * bidirectional attention\n",
    "        * no text generation\n",
    "    * decoder\n",
    "        * causal masking for attention\n",
    "        * text generation\n",
    "\n",
    "## Applications for language translation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
