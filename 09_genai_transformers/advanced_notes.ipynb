{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced concepts of transformer architecture\n",
    "\n",
    "### Decoder models\n",
    "\n",
    "* transforsmers were developed for language translation, they usually consist of two parts\n",
    "    * encoder -> process input data (source language sentence)\n",
    "    * decoder -> generates output data (translated sentence)\n",
    "* over time, decodes have become central to text generation tasks, forming basis of models like GPT, LLaMA\n",
    "\n",
    "* decoder model\n",
    "    * generative pre-training - > model is trained to predict the next word in a sequence based on previous words\n",
    "    * autoregressive models -> sequentially generating text, predicting text on words that came before\n",
    "\n",
    "* fine-tuning & reinforcement learning\n",
    "    * after initial training, GPT models can be fine-tuned for specific tasks (question answering or classification), model is trained on labeled data (supervised learning)\n",
    "    * reinforcement learning from human feedback is a fine-tuning method where human feedback is used to improve the performance, especially in apps like chatbots\n",
    "\n",
    "* decoders in text generation\n",
    "    * key difference from translation is that decoders rely on input from encoders (using cross-attention), in text generation they are independent, predicting next word based on preciding sequence\n",
    "    * autoregressive process starts with begining-of-sentence token, it predicts the next word, and appends it to the sequence.\n",
    "\n",
    "* masked self-attention in decoders hiddes future tokens\n",
    "\n",
    "* text generation process\n",
    "    * input prompt\n",
    "    * tokenization\n",
    "    * word embeddings\n",
    "    * positional encoding\n",
    "    * contextual embeddings\n",
    "    * logits\n",
    "    * argmax\n",
    "    * appending & repeat\n",
    "    * generation stops once end-of-sequence token or token limit is hit\n",
    "\n",
    "### Encoder models\n",
    "\n",
    "### Applications for language translation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
