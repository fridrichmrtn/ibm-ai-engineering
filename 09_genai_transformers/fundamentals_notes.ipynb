{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamental concepts of transformer architecture\n",
    "\n",
    "### Positional encoding\n",
    "* position of a token is important to reflect meaning of a sentence\n",
    "* sin & cos waves\n",
    "    * $PE(pos,2i) = sin(\\frac{pos}{10000^\\frac{2i}{d_{model}}})$ (even dimensions)\n",
    "    * $PE(pos,2i+1) = cos(\\frac{pos}{10000^\\frac{2i}{d_{model}}})$ (odd dimensions)\n",
    "    * where $pos$ represents position of $sin$ wave over time\n",
    "    * $i$ is dimension index and helps to generate unique sin or cos wave for each embedding\n",
    "    * $0 \\leq i < \\frac{d}{2}$\n",
    "* example\n",
    "    * Sentence \"Transformers are awesome\"\n",
    "\n",
    "    **Embeddings**\n",
    "\n",
    "| Token | D1 | D2 | D3 | D4 |\n",
    "| -------- | ------- |------- |------- |------- |\n",
    "| Transformers | 0.2 | 0.4 | 0.1 | 0.3 |\n",
    "| are | 0.5 | 0.2 | 0.7 | 0.9 |\n",
    "| awesome | 0.6 | 0.6 | 0.4 | 0.2 |\n",
    "\n",
    "Position (Pos) -> 0,1,2  \n",
    "Dimensions (i) -> 0,1,2,3  \n",
    "\n",
    "\n",
    "**Example calculation**\n",
    "\n",
    "$PE(0,0) = sin(0/10000^{(2*0/4)}) = sin(0) = 0$  \n",
    "$PE(0,0) = cos(0/10000^{(2*0/4)}) = cos(0) = 1$  \n",
    "$PE(0,1) = sin(0/10000^{(2*0/4)}) = sin(0) = 0$  \n",
    "$PE(0,1) = cos(0/10000^{(2*0/4)}) = cos(0) = 1$  \n",
    "\n",
    "**Positional encodings**\n",
    "\n",
    "| Token | D1 | D2 | D3 | D4 |\n",
    "| -------- | ------- |------- |------- |------- |\n",
    "| Transformers | 0 | 1 | 0 | 1 |\n",
    "| are | 0.84 | 0.54 | 0.01 | 0.99 |\n",
    "| awesome | 0.90 | -0.41 | 0.02 | 0.99 |\n",
    "\n",
    "* you can limit sequence size by vocabulary size, the encoding matrix is then (vocab size) x (embedding size)\n",
    "* columns here are indicative of varying position function, while rows are representative of a position in a sequence\n",
    "* this generally enables to the positional encoding to be unique (due to the numerous positioning funcs)\n",
    "* positional encoding can be added to the embeddings vector, ensuring the elements' order is maintained\n",
    "* positional encoding can be learnable (GPT)\n",
    "* segment embeddings are added in some models (BERT), providing additional positional information\n",
    "\n",
    "### Attention (translation example)\n",
    "\n",
    "* query, dictionary {key, value}; \n",
    "* query -> one-hot encoded vectors, composing a query matrix\n",
    "* keys -> one-hot encoded vectors, composing a key matrix\n",
    "* values -> one-hot encoded vectors, composing values mat\n",
    "* rows of keys and values needs to be aligned (row for a key corresponds with the row for the values)\n",
    "* $Attention (q_{sous}, K, V) = q_{sous}^T \\ K^T \\ V$, where we query a key matrix and retrieve the value based on the key\n",
    "    * $h = q_{word}^T \\ K^T \\ V$\n",
    "    * retrieving translated word with $\\hat w = argmax_i\\{hV^T\\}$\n",
    "* can be expanded to word embeddings, that is one can use embedding representation rather the one-hot encoding, which allows for dealing with an unseen words\n",
    "* the approach can be further refined by incorporating softmax to the first part of the search\n",
    "* $Attention (q_{ci-dessous, K, V}) = softmax_r (q_{ci-dessous}^T \\ K^T) V$\n",
    "    * retrieving translated word again with $\\hat w = argmax_i\\{hV^T\\}$\n",
    "\n",
    "### Self-attention mechanism (translation example)\n",
    "\n",
    "* core of a language transformer,\n",
    "* each word in a sequence attends every other word in parallel\n",
    "* each sequence transformed to a matrix representation, where the sequence length is not fixed\n",
    "    * query projections with bias $Q = W_qX+b_q1^T$, where $W$ and $b$ are learnable params\n",
    "    * key projections with bias $K = W_kX+b_k1^T$, with similar learnable params\n",
    "    * value projections with bias $V = W_vX+b_v1^T$, with similar learnable params\n",
    "    * sequential embeddings are usually added before this operation\n",
    "* $Attention (Q, K, V) = V softmax_c(\\frac{K^T \\ Q}{\\sqrt D})$\n",
    "    * $H' =  V softmax_c(\\frac{K^T \\ Q}{\\sqrt D})$, where $D$ represents the dimension of the embeddings\n",
    "    * length of $H'$ consists of enhanced embeddings (columns), and their number is aligned with the input sequence\n",
    "    * additional learnable output projection can be used such as $H = W^{[0]} H' + b^{[0]}1^T$, using additional learnable params\n",
    "* averaged contextual embeddings are further passed to simple neural net\n",
    "* its initial layer output is $z^1 = \\frac{1}{T} \\sum_{t=1}^T \\ h_t$ (average embedding), dimensions aligned with the embeddings\n",
    "* outputs logits $z^{2}$ that are aligned with the vocab size\n",
    "* $z^{[2]} = W^{[2]}z^{[1]}+b^{[2]}$\n",
    "* prediction are produced with $\\hat w_t = argmax_i(z^{[2]})$, the dot-product in the previously described approach is replaced by the simple neural net\n",
    "\n",
    "### Transformers\n",
    "\n",
    "* scaled dot-product with multiple heads\n",
    "    * foundational piece of transformer models\n",
    "    * the core mechanism computes a dot product between queries and keys, scales the result (to ensure numerical stability), applies softmax to get attention weights\n",
    "    * in some cases masking is incorporated to hide future words\n",
    "    * attention weights are multiplied by the values for the final output\n",
    "    * self-attention -> Q, K, V all come from same input, which is derived by learnable matrices\n",
    "    * cross-attention -> Q comes from one input (source lang), K and V come from another (target language)\n",
    "\n",
    "* multi-head attention\n",
    "    * instead of computing a single attention output, multihead attention computes multiple outputs in parallel\n",
    "    * process\n",
    "        * splitting inputs -> input embeddings are split into smaller vectors\n",
    "        * parallel attention -> each split vector is processed by its own attention mechanism\n",
    "        * concatenation -> outputs of all heads are concatenated and passed through a final linear layer to produce output\n",
    "    * benefits -> each head can focus on different parts of the input seq, capturing diverse dependencies (subject-verb relationship vs adjectives)\n",
    "* transformer architecture\n",
    "    * the core idea is improve efficiency by stacking multiple layers of attention and feed-forward nets together\n",
    "    * components\n",
    "        * encoder -> processes the input seq using multihead self-attention, followed by a feed-forward net\n",
    "            * multi-head attention computes attention over the input\n",
    "            * add & norm adds the attention output to the original input and applies layer normalization\n",
    "            * feed-forward net is applied to each position independently\n",
    "        * decoder\n",
    "            * similar to encoder, but includes masking to prevent leakage\n",
    "            * uses cross-attention layers for translation\n",
    "    * stacking\n",
    "        * multiple enc and dec layers can be stacked to capture complex relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers for classification (encoder)\n",
    "\n",
    "* transformers for text classification\n",
    "    * transformers can leverage word-ordering and contextual information in the classification task\n",
    "    * attention mechanism might be helpful here :)\n",
    "\n",
    "* creating the text pipe\n",
    "    * iterators\n",
    "    * data split\n",
    "    * tokenization to split sentences into words and to build vocabulary\n",
    "    * padding to standardize input size\n",
    "    * data loader objects for train, valid, test; labels and sequences\n",
    "\n",
    "* creating the model\n",
    "    * embedding layers -> converting input indices into embeddings\n",
    "    * positional encoding -> positional encodings informs the encoder of the ordering, allows the model to understand both order and semantic meaning\n",
    "    * transformer encoder -> multiple encoder layers stacked together, each layer consists of self-attention and ffn, outputs contextual embedding\n",
    "    * classification layer -> output layer \n",
    "\n",
    "* model forward method\n",
    "    * input tensor -> [seq, batch]\n",
    "    * embedding layer -> input converted into shape [seq, batch, emb_dim]\n",
    "    * positional encoding -> added\n",
    "    * transformer encoder -> processes emb+pos into contextual vecs\n",
    "    * mean pooling -> averaging contextual embeddings into one\n",
    "    * classifier -> linear layer predicts the labels\n",
    "\n",
    "* training as usual\n",
    "    * cross-entropy loss, SGD, learning rate scheduler\n",
    "\n",
    "* practical considerations\n",
    "    * padding -> make sure all sequences have same length\n",
    "    * positional encoding -> essential to understand word order\n",
    "    * mean-pooling -> condenses the embeddings into a single vector\n",
    "    * hyper-params -> number of encoding layers, embedding dimension, number of attention heads."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
