{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning\n",
    "\n",
    "\n",
    "## Hugging face & Pytorch\n",
    "\n",
    "#### **Hugging Face**\n",
    "- A platform for **machine learning (ML)** and **Natural Language Processing (NLP)**.\n",
    "- Known for its **Transformers Library**, offering pre-trained models like **BERT**, **GPT**, and **T5**.\n",
    "- Focuses on **NLP tasks** such as text classification, sentiment analysis, and text generation.\n",
    "- Often called the **\"GitHub of Machine Learning\"** due to its collaborative, community-driven approach.\n",
    "- Ideal for quick, out-of-the-box NLP solutions.\n",
    "\n",
    "#### **PyTorch**\n",
    "- An **open-source deep learning framework** developed by **Meta**.\n",
    "- Built on **Python**, making it intuitive and widely used in research and academia.\n",
    "- Key feature: **dynamic computation graphs**, allowing real-time changes to models.\n",
    "- Excels in **custom model building**, rapid prototyping, and handling large-scale computations with GPU acceleration.\n",
    "- Best for creating tailored, research-focused AI solutions.\n",
    "\n",
    "\n",
    "#### **Summary**\n",
    "- **Hugging Face** simplifies NLP with pre-trained models and tools, while **PyTorch** provides flexibility for custom model development.\n",
    "- Together, they integrate seamlessly for advanced NLP applications like sentiment analysis, language translation, and text summarization.\n",
    "- Hugging Face is great for quick implementations, while PyTorch is ideal for research and custom solutions.\n",
    "\n",
    "## Fine-tuning\n",
    "\n",
    "* training LLMs from scratch requires significant time, infrastructure, dataset & investment, fine-tuning adapts general models for downstream tasks such as sentiment analysis or text generation using domain datasets\n",
    "\n",
    "* benefits\n",
    "    * time & resource efficiency\n",
    "    * tailored responses\n",
    "    * task-specific adaptation\n",
    "* pitfalls\n",
    "    * overfitting & underfitting\n",
    "    * catastrophic forgetting (losing initial knowledge)\n",
    "    * data leakage\n",
    "\n",
    "* QA bot example\n",
    "    * domain specific dataset\n",
    "    * novel cost function\n",
    "    * reinforcement learning, direct preference optimization, encoder as evaluator\n",
    "    * response evaluation is hard => adapting BERT to produce continuous outputs that serve as reward modeling\n",
    "\n",
    "* fine-tuning strategies\n",
    "    * self-supervised fine-tuning (masking & predicting words)\n",
    "    * supervised fine-tuning (sentiment prediction)\n",
    "    * reinforcement learning from human feedback\n",
    "        * prompt-> model -> response 1, response 2, response 3 -> human ranking -> reward -> model\n",
    "    * hybrid fine-tuning (combining multiple approaches)\n",
    "\n",
    "* direct preference optimization\n",
    "    * optimizes language models directly based on human preferences\n",
    "    * simple -> more easy to implement than RLHF\n",
    "    * human-centric -> aligns model outputs with human preference\n",
    "    * no reward training necessary\n",
    "    * faster convergence\n",
    "\n",
    "* supervised fine-tuning\n",
    "    * full fine-tuning (all parameters are tuned)\n",
    "    * parameter efficient fine-tuning (most parameters are kept)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter efficient fine-tuning\n",
    "\n",
    "* supervised fine-tuning -> using pre-trained model, modifying the gained knowledge to the task at hand\n",
    "* full fine-tuning -> updating learning params, layers and neurons, compute, memory, and data intensive\n",
    "* parameter efficient fine-tuning -> limited number of trainable params, less comp, mem & data intensive\n",
    "\n",
    "    * selective\n",
    "        * updates only subset of layers/params, not suitable for transformers\n",
    "\n",
    "    * additive\n",
    "        * adding new task-specific layers/components to the pre-trained models\n",
    "        * adapters -> additional layers on top of attention and feed-forward parts\n",
    "            * adapters creates internal bottleneck for compression, non-linear transformation and upscale the results back\n",
    "        * soft prompts\n",
    "            * improve training process\n",
    "            * learnable tensors concatenated with the input embedding\n",
    "            * methods -> prompt tuning, prefix tuning, p-tuning and multitask prompt tuning\n",
    "\n",
    "    * reparametrization\n",
    "        * rank\n",
    "            * minimum number of vectors needed to span a space (dimension)\n",
    "            * the space can be reduced to lower dimensions which can make models more efficient\n",
    "        * Low-Rank Adaptation -> uses reparametrizing network weights using low-rank transformations\n",
    "        * Quantized Low-Rank Adaptation -> reduces memory footprint and computational requirements\n",
    "        * Weight-decomposed Low-Rank Adaptation -> adjusts the rank of the low-rank space based on component magnitude\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Low-Rank Adaptation\n",
    "\n",
    "* simplifies large and complex machine learning models, adds light-weight add-ins for efficient function\n",
    "* reduces trainable params of pre-trained models; decreases training time, resource usage and memory\n",
    "* $h(x) =  W_0x+\\frac{\\alpha}{r}\\ BAx$, where the second term describes the decomposition of the $\\delta W$ into lower rank representation and is scaled by the factor of $\\frac{\\alpha}{r}$, $r$ stands for rank, $W_0$ is fixed param\n",
    "* used in attention piece, also for Q,K,V matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## From Quantization to QLoRA\n",
    "\n",
    "* quantized low-rank adaptation, optimizing LLMs performance and efficiency\n",
    "* reduces the precision of weight representation to finite number of discrete levels\n",
    "* quantization\n",
    "    * power of 2, 3-bit quantization means 8 levels, 4-bit means 16 discrete levels, etc\n",
    "    * 4-bit NF, double quantization\n",
    "    * allows models to fit into limited memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Ethical Considerations in Fine-Tuning Large Language Models\n",
    "\n",
    "Fine-tuning large language models (LLMs) raises several ethical concerns, especially as these models grow in capability and usage across diverse applications. Here, weâ€™ll explore key ethical considerations that developers and organizations should keep in mind when fine-tuning LLMs.\n",
    "\n",
    "## Bias Amplification\n",
    "One of the primary concerns with LLMs is the amplification of biases present in training data.\n",
    "\n",
    "- **Understanding Bias in Data**: Language models learn patterns from vast datasets, which often contain societal biases related to gender, race, or ethnicity. These biases can lead to skewed or unfair model outputs.\n",
    "- **Debiasing Techniques**: Various techniques, such as adjusting word embeddings or filtering out biased data, can help reduce these biases. Regular evaluation and monitoring of outputs during fine-tuning can help identify areas where bias may emerge.\n",
    "\n",
    "## Data Privacy\n",
    "LLMs trained on proprietary or sensitive data can unintentionally memorize and reproduce specific details, leading to potential privacy issues.\n",
    "\n",
    "- **Incorporating Differential Privacy**: Techniques like differential privacy introduce noise to data, ensuring that individual details are not retained, thereby protecting user information.\n",
    "- **Data Anonymization**: Removing or anonymizing identifiable information from datasets before fine-tuning can help minimize the risk of data leakage in generated outputs.\n",
    "\n",
    "## Environmental Impact\n",
    "Training and fine-tuning LLMs is computationally intensive, contributing significantly to energy consumption and carbon emissions.\n",
    "\n",
    "- **Energy-Efficient Training Methods**: Techniques like parameter-efficient fine-tuning (PEFT) and model distillation allow for effective fine-tuning with less computational resource usage, helping to reduce environmental impact.\n",
    "- **Carbon Offset Initiatives**: Some organizations offset emissions by investing in renewable energy sources or supporting environmental programs, helping to balance the ecological footprint associated with large-scale model training.\n",
    "\n",
    "## Transparency and Accountability\n",
    "Transparent communication about model capabilities and limitations is essential for responsible AI deployment.\n",
    "\n",
    "- **Model Documentation**: Documenting the fine-tuning process, including the data sources and any modifications made, allows users to understand the context and scope of the model.\n",
    "- **Usage Guidelines**: Clearly defined usage guidelines ensure users are aware of how the model has been fine-tuned and provide context for responsible application.\n",
    "\n",
    "## Ensuring Fair Representation\n",
    "Large models should be inclusive of diverse demographics to avoid exclusionary or biased results.\n",
    "\n",
    "- **Dataset Diversity**: Use datasets that represent various demographics, cultures, and languages to create a more balanced model.\n",
    "- **Regular Evaluation**: Periodic checks and updates to the model based on emerging data or feedback from diverse user groups help maintain fairness and inclusivity in model outputs.\n",
    "\n",
    "## Conclusion\n",
    "Addressing these ethical considerations in fine-tuning LLMs fosters a more responsible approach to AI development. By proactively implementing bias mitigation, data privacy, and transparency practices, developers can build LLMs that are not only effective but also socially responsible and trustworthy."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
