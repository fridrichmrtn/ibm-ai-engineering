{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning\n",
    "\n",
    "\n",
    "## Hugging face & Pytorch\n",
    "\n",
    "#### **Hugging Face**\n",
    "- A platform for **machine learning (ML)** and **Natural Language Processing (NLP)**.\n",
    "- Known for its **Transformers Library**, offering pre-trained models like **BERT**, **GPT**, and **T5**.\n",
    "- Focuses on **NLP tasks** such as text classification, sentiment analysis, and text generation.\n",
    "- Often called the **\"GitHub of Machine Learning\"** due to its collaborative, community-driven approach.\n",
    "- Ideal for quick, out-of-the-box NLP solutions.\n",
    "\n",
    "#### **PyTorch**\n",
    "- An **open-source deep learning framework** developed by **Meta**.\n",
    "- Built on **Python**, making it intuitive and widely used in research and academia.\n",
    "- Key feature: **dynamic computation graphs**, allowing real-time changes to models.\n",
    "- Excels in **custom model building**, rapid prototyping, and handling large-scale computations with GPU acceleration.\n",
    "- Best for creating tailored, research-focused AI solutions.\n",
    "\n",
    "\n",
    "#### **Summary**\n",
    "- **Hugging Face** simplifies NLP with pre-trained models and tools, while **PyTorch** provides flexibility for custom model development.\n",
    "- Together, they integrate seamlessly for advanced NLP applications like sentiment analysis, language translation, and text summarization.\n",
    "- Hugging Face is great for quick implementations, while PyTorch is ideal for research and custom solutions.\n",
    "\n",
    "## Fine-tuning\n",
    "\n",
    "* training LLMs from scratch requires significant time, infrastructure, dataset & investment, fine-tuning adapts general models for downstream tasks such as sentiment analysis or text generation using domain datasets\n",
    "\n",
    "* benefits\n",
    "    * time & resource efficiency\n",
    "    * tailored responses\n",
    "    * task-specific adaptation\n",
    "* pitfalls\n",
    "    * overfitting & underfitting\n",
    "    * catastrophic forgetting (losing initial knowledge)\n",
    "    * data leakage\n",
    "\n",
    "* QA bot example\n",
    "    * domain specific dataset\n",
    "    * novel cost function\n",
    "    * reinforcement learning, direct preference optimization, encoder as evaluator\n",
    "    * response evaluation is hard => adapting BERT to produce continuous outputs that serve as reward modeling\n",
    "\n",
    "* fine-tuning strategies\n",
    "    * self-supervised fine-tuning (masking & predicting words)\n",
    "    * supervised fine-tuning (sentiment prediction)\n",
    "    * reinforcement learning from human feedback\n",
    "        * prompt-> model -> response 1, response 2, response 3 -> human ranking -> reward -> model\n",
    "    * hybrid fine-tuning (combining multiple approaches)\n",
    "\n",
    "* direct preference optimization\n",
    "    * optimizes language models directly based on human preferences\n",
    "    * simple -> more easy to implement than RLHF\n",
    "    * human-centric -> aligns model outputs with human preference\n",
    "    * no reward training necessary\n",
    "    * faster convergence\n",
    "\n",
    "* supervised fine-tuning\n",
    "    * full fine-tuning (all parameters are tuned)\n",
    "    * parameter efficient fine-tuning (most parameters are kept)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
