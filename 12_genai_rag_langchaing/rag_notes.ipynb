{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "687a8213",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation\n",
    "\n",
    "## Intro to RAG\n",
    "\n",
    "* RAG is a framework that combines\n",
    "    * a pre-trained LLM (the generator)\n",
    "    * an external knowledge source (retriever) to provide accurate domain-specific information without retraining the LLM\n",
    "    * why?\n",
    "        * LLMs lacks domain specific knowledge\n",
    "        * LLMs can be separated to internal and/or confidential data (ie querying company's travel policy)\n",
    "\n",
    "* RAG Workflow\n",
    "    * text embedding\n",
    "        * convert the user's prompt/question into a high-dim vec using a question encoder\n",
    "        * convert each text chunk of the knowledge base into a high-dim vec using a context encoder\n",
    "    * retrieval\n",
    "        * compare the prompt vector to vectors in the knowledge base\n",
    "        * identify the top-k matches\n",
    "    * augmented query creation\n",
    "        * combine the retrieved text chunks with the original prompt to form an augmented query\n",
    "    * model generation\n",
    "        * LLM uses the augmented query to produce a contextually relevant response\n",
    "\n",
    "* Details\n",
    "    * prompt encoding\n",
    "        * use token embeddings to convert words/sub-words into vectors\n",
    "        * average those token vectors to get a single vector representation of the prompt\n",
    "    * context encoding\n",
    "        * split large docs into smaller chunks\n",
    "        * embed each chunk separately\n",
    "        * store these chunk embeddings in a vector database\n",
    "    * retrieval\n",
    "        * the system computes similarity/distance between the prompt vector and each chunk vector\n",
    "        * dot product focuses on magnitude and direction vs cosine sim focuses on direction only\n",
    "        * select top-k chunks with minimal distance\n",
    "    * final response\n",
    "        * combine the retrieved text and the original question into the LLM to generate a domain-specific, accurate response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25977cf6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## RAG Encoders and FAISS\n",
    "\n",
    "* Context encoder\n",
    "    * context encoder + tokenizer -> encodes large test passages into dense vector embeddings\n",
    "    * process\n",
    "        * tokenize text passages (paragraphs)\n",
    "        * use pre-traied DPR context encoder to create embeddings\n",
    "        * store these embeddings in a vector database\n",
    "\n",
    "* FAISS\n",
    "    * a library for efficient similarity search on large sets of high-dim vectors\n",
    "    * process\n",
    "        * convert embeddings to a numpy float32 arr\n",
    "        * initialize FAISS index (eg for L2 distance)\n",
    "        * add context embeddings into FAISS for fast lookups\n",
    "\n",
    "* Question encoder\n",
    "    * DPR question encoder + tokenizer, similar to context encoder but for questions, returns question embedding\n",
    "\n",
    "* Retrieval w/ FAISS\n",
    "    * encode the question\n",
    "    * compute distances to all store context embeddings\n",
    "    * return top-k closest matches\n",
    "    \n",
    "* Response generation with decoder\n",
    "    * wo/ context relying only on the pre-trained model\n",
    "    * w/ augmentation\n",
    "        * retrieve top-k passage from FAISS\n",
    "        * concatenate the passages with the question\n",
    "        * feed that combined input into the generator\n",
    "        * generates a final, context-informed answer"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
