{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gen AI Overview and Architecture\n",
    "\n",
    "Architectures\n",
    "* Recurrent Neural Networks\n",
    "    * architecture reflects natural order & time-based dependencies, sequence-based dependencies\n",
    "    * language representation\n",
    "    * fine-tuning needed for handling a specific task\n",
    "    * examples: NLP, translations, speech recognition, image captioning\n",
    "* Transformers\n",
    "    * self-attention mechanism reflecting relationship between entities (words), and enables to focus on the most important parts of a relationship, making the internal decision making efficient\n",
    "    * enables parallelization, can process longer contexts/relationships than RNNs\n",
    "    * fine-tuning, the model remains intact for the most part, only output layers tuned\n",
    "    * examples: text generation (GPT)\n",
    "* Generative Advesarial Networks\n",
    "    * two sub-components, generator (fake samples) vs discriminator (identify fake samples)\n",
    "    * adversarial process,\n",
    "    * examples: image and video generation\n",
    "* Variational AutoEncoders\n",
    "    * encoder, compressed representation, decoder\n",
    "    * learning patterns within the data\n",
    "    * examples: art and creative design\n",
    "* Diffusion models\n",
    "    * generates images by removing noise/reconstruct distorted data\n",
    "    * examples: image/video processing\n",
    "\n",
    "* Others\n",
    "    * Reinforcement Learning\n",
    "        * agents learn from interaction with environment to maximize rewards\n",
    "        * in GenAI used for fine-tuning, and further optimization of the models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
